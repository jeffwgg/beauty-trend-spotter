{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1afeebc2",
   "metadata": {
    "papermill": {
     "duration": 0.002576,
     "end_time": "2025-08-25T05:22:52.406618",
     "exception": false,
     "start_time": "2025-08-25T05:22:52.404042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load, Clean & Merge Comment + Video Data (Chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff87cd3b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-25T05:22:52.411747Z",
     "iopub.status.busy": "2025-08-25T05:22:52.411491Z",
     "iopub.status.idle": "2025-08-25T05:22:53.840047Z",
     "shell.execute_reply": "2025-08-25T05:22:53.839252Z"
    },
    "papermill": {
     "duration": 1.432627,
     "end_time": "2025-08-25T05:22:53.841495",
     "exception": false,
     "start_time": "2025-08-25T05:22:52.408868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def read_comments(pattern, usecols=None, chunksize=200_000):\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        print(f\"[ERROR] No files matched {pattern}\")\n",
    "        return pd.DataFrame()\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        print(f\"[INFO] Reading {f}\")\n",
    "        for chunk in pd.read_csv(f, usecols=usecols, chunksize=chunksize):\n",
    "            frames.append(chunk)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def clean_text(s: pd.Series) -> pd.Series:\n",
    "    s = s.fillna(\"\")\n",
    "    s = s.str.replace(r\"http\\S+|www\\.\\S+\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    return s\n",
    "\n",
    "def run_pipeline(comments_glob, videos_csv, out_path, chunksize=200_000):\n",
    "    comments_cols = [\"commentId\",\"videoId\",\"textOriginal\",\"likeCount\",\"publishedAt\"]\n",
    "    videos_cols   = [\"videoId\",\"title\",\"description\",\"channelId\",\"category\",\"publishedAt\"]\n",
    "\n",
    "    comments = read_comments(comments_glob, usecols=comments_cols, chunksize=chunksize)\n",
    "    print(f\"[INFO] Comments loaded: {len(comments):,} rows\")\n",
    "\n",
    "    videos = pd.read_csv(videos_csv, usecols=lambda c: c in videos_cols)\n",
    "    print(f\"[INFO] Videos loaded: {len(videos):,} rows\")\n",
    "\n",
    "    # type conversions\n",
    "    comments[\"likeCount\"] = pd.to_numeric(comments[\"likeCount\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    comments[\"publishedAt\"] = pd.to_datetime(comments[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "    comments[\"textOriginal\"] = clean_text(comments[\"textOriginal\"])\n",
    "\n",
    "    if \"publishedAt\" in videos.columns:\n",
    "        videos[\"publishedAt\"] = pd.to_datetime(videos[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    merged = comments.merge(videos, on=\"videoId\", how=\"left\", suffixes=(\"_comment\",\"_video\"))\n",
    "    print(f\"[INFO] Final shape: {merged.shape}\")\n",
    "\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    merged.to_parquet(out_path, engine=\"pyarrow\", index=False)\n",
    "    print(f\"[OK] Saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c7fe05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T05:22:53.846742Z",
     "iopub.status.busy": "2025-08-25T05:22:53.846153Z",
     "iopub.status.idle": "2025-08-25T05:22:53.849717Z",
     "shell.execute_reply": "2025-08-25T05:22:53.849072Z"
    },
    "papermill": {
     "duration": 0.00718,
     "end_time": "2025-08-25T05:22:53.850854",
     "exception": false,
     "start_time": "2025-08-25T05:22:53.843674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMMENTS_GLOB = \"/kaggle/input/datathon-loreal/comments*.csv\"  # matches comments1.csv ... comments5.csv\n",
    "VIDEOS_CSV    = \"/kaggle/input/datathon-loreal/videos.csv\"\n",
    "OUT_PATH      = \"/kaggle/working/comments_merged.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef499671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T05:22:53.855290Z",
     "iopub.status.busy": "2025-08-25T05:22:53.855082Z",
     "iopub.status.idle": "2025-08-25T05:24:06.067705Z",
     "shell.execute_reply": "2025-08-25T05:24:06.066730Z"
    },
    "papermill": {
     "duration": 72.216515,
     "end_time": "2025-08-25T05:24:06.069215",
     "exception": false,
     "start_time": "2025-08-25T05:22:53.852700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading /kaggle/input/datathon-loreal/comments1.csv\n",
      "[INFO] Reading /kaggle/input/datathon-loreal/comments2.csv\n",
      "[INFO] Reading /kaggle/input/datathon-loreal/comments3.csv\n",
      "[INFO] Reading /kaggle/input/datathon-loreal/comments4.csv\n",
      "[INFO] Reading /kaggle/input/datathon-loreal/comments5.csv\n",
      "[INFO] Comments loaded: 4,725,012 rows\n",
      "[INFO] Videos loaded: 92,759 rows\n",
      "[INFO] Final shape: (4725012, 9)\n",
      "[OK] Saved to /kaggle/working/comments_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "CHUNKSIZE = 200_000\n",
    "run_pipeline(COMMENTS_GLOB, VIDEOS_CSV, OUT_PATH, chunksize=CHUNKSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0fad4e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T05:24:06.074887Z",
     "iopub.status.busy": "2025-08-25T05:24:06.074651Z",
     "iopub.status.idle": "2025-08-25T05:24:06.247281Z",
     "shell.execute_reply": "2025-08-25T05:24:06.246462Z"
    },
    "papermill": {
     "duration": 0.177262,
     "end_time": "2025-08-25T05:24:06.248965",
     "exception": false,
     "start_time": "2025-08-25T05:24:06.071703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.4G\r\n",
      "-rw-r--r-- 1 root root 1.4G Aug 25 05:24 comments_merged.parquet\r\n",
      "---------- 1 root root  15K Aug 25 05:24 __notebook__.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19781cab",
   "metadata": {
    "papermill": {
     "duration": 0.002405,
     "end_time": "2025-08-25T05:24:06.255103",
     "exception": false,
     "start_time": "2025-08-25T05:24:06.252698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Normalize Text, Extract Hashtags & Emojis, Add Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea714d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T05:24:06.261633Z",
     "iopub.status.busy": "2025-08-25T05:24:06.261336Z",
     "iopub.status.idle": "2025-08-25T05:24:14.844732Z",
     "shell.execute_reply": "2025-08-25T05:24:14.843923Z"
    },
    "papermill": {
     "duration": 8.588556,
     "end_time": "2025-08-25T05:24:14.846213",
     "exception": false,
     "start_time": "2025-08-25T05:24:06.257657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    }
   ],
   "source": [
    "# --- deps (only first run needs to download) ---\n",
    "!pip -q install ftfy langdetect emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ca8499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T05:24:14.853001Z",
     "iopub.status.busy": "2025-08-25T05:24:14.852443Z",
     "iopub.status.idle": "2025-08-25T05:24:15.006725Z",
     "shell.execute_reply": "2025-08-25T05:24:15.005868Z"
    },
    "papermill": {
     "duration": 0.159251,
     "end_time": "2025-08-25T05:24:15.008310",
     "exception": false,
     "start_time": "2025-08-25T05:24:14.849059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from langdetect import detect, DetectorFactory\n",
    "import ftfy\n",
    "import emoji as emoji_lib  # renamed to avoid conflict\n",
    "\n",
    "# Reproducible language detection\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "# ====== CONFIG (edit as needed) ======\n",
    "INPUT  = \"/kaggle/working/comments_merged.parquet\"\n",
    "OUTPUT = \"/kaggle/working/comments_enriched.parquet\"\n",
    "ADD_LANG = True  # Set to False to skip language detection (faster)\n",
    "# =====================================\n",
    "\n",
    "# Pre-compile regex patterns for efficiency\n",
    "URL_RE     = re.compile(r\"http\\S+|www\\.\\S+\")           # Match URLs\n",
    "HASHTAG_RE = re.compile(r\"(#\\w+)\")                    # Match hashtags like #Trending\n",
    "MENTION_RE = re.compile(r\"@\\w+\")                      # Match @mentions\n",
    "EMOJI_RE   = re.compile(\"[\\U0001F600-\\U0001F64F]+\")   # Match emoticons (faces, etc.)\n",
    "\n",
    "def norm(t):\n",
    "    \"\"\"Normalize text: fix encoding, remove URLs, clean whitespace\"\"\"\n",
    "    t = ftfy.fix_text(str(t))\n",
    "    t = URL_RE.sub(\" <URL> \", t)\n",
    "    t = MENTION_RE.sub(\" <MENTION> \", t)\n",
    "    t = re.sub(r\"[^\\w\\s\\#\\.\\!\\?\\U0001F600-\\U0001F64F]\", \" \", t, flags=re.UNICODE)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def extract_hashtags(t):\n",
    "    \"\"\"Extract all hashtags, lowercase them\"\"\"\n",
    "    t = t or \"\"\n",
    "    return [h.lower() for h in HASHTAG_RE.findall(t)]\n",
    "\n",
    "def count_emojis(t):\n",
    "    \"\"\"Count number of emoji characters in text\"\"\"\n",
    "    t = t or \"\"\n",
    "    return len(EMOJI_RE.findall(t))\n",
    "\n",
    "def detect_lang_safe(t):\n",
    "    \"\"\"Safely detect language with fallback\"\"\"\n",
    "    try:\n",
    "        t = (t or \"\").strip()\n",
    "        if len(t) < 3:\n",
    "            return \"unk\"\n",
    "        return detect(t)\n",
    "    except Exception:\n",
    "        return \"unk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3889048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T05:24:15.014421Z",
     "iopub.status.busy": "2025-08-25T05:24:15.014183Z",
     "iopub.status.idle": "2025-08-25T10:27:27.246825Z",
     "shell.execute_reply": "2025-08-25T10:27:27.246122Z"
    },
    "papermill": {
     "duration": 18192.237106,
     "end_time": "2025-08-25T10:27:27.248226",
     "exception": false,
     "start_time": "2025-08-25T05:24:15.011120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data from /kaggle/working/comments_merged.parquet\n",
      "[INFO] Loaded: 4,725,012 rows | 9 columns\n",
      "[INFO] Applying text normalization and feature extraction...\n",
      "[INFO] Detecting language (this may take a few minutes)...\n",
      "[INFO] Adding time-based features...\n",
      "[INFO] Dropped 0 rows with invalid timestamps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1524636790.py:36: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df[\"week_start\"] = df[tcol].dt.to_period(\"W-MON\").apply(lambda p: p.start_time.date())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving enriched data to /kaggle/working/comments_enriched.parquet\n",
      "[OK] Enrichment complete → /kaggle/working/comments_enriched.parquet | Final shape: (4725012, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_norm</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLEASE LESBIAN FLAG I BEG YOU You would rock it</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-08-15</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apply mashed potato juice and mixed it with curd</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2023-10-02</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69 missed calls from mars</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baaa</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>so</td>\n",
       "      <td>2024-02-13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you look like raven from phenomena raven no cap</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>sl</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_norm hashtags  emoji_count  \\\n",
       "0   PLEASE LESBIAN FLAG I BEG YOU You would rock it       []            0   \n",
       "1  Apply mashed potato juice and mixed it with curd       []            0   \n",
       "2                         69 missed calls from mars       []            0   \n",
       "3                                              Baaa       []            0   \n",
       "4   you look like raven from phenomena raven no cap       []            0   \n",
       "\n",
       "  lang        date  hour  \n",
       "0   en  2023-08-15    21  \n",
       "1   en  2023-10-02    13  \n",
       "2   en  2024-05-31    12  \n",
       "3   so  2024-02-13    15  \n",
       "4   sl  2020-02-15    22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- RUN ENRICHMENT ----\n",
    "print(f\"[INFO] Loading data from {INPUT}\")\n",
    "df = pd.read_parquet(INPUT)\n",
    "print(f\"[INFO] Loaded: {len(df):,} rows | {df.shape[1]} columns\")\n",
    "\n",
    "# Handle timestamp column (support both naming conventions)\n",
    "tcol = \"publishedAt_comment\" if \"publishedAt_comment\" in df.columns else \"publishedAt\"\n",
    "if tcol not in df.columns:\n",
    "    raise ValueError(f\"Timestamp column '{tcol}' not found in data\")\n",
    "\n",
    "# Handle text source\n",
    "text_col = \"textOriginal\" if \"textOriginal\" in df.columns else \"text\"\n",
    "if text_col not in df.columns:\n",
    "    raise ValueError(f\"Text column '{text_col}' not found in data\")\n",
    "\n",
    "# Apply text enrichment\n",
    "print(\"[INFO] Applying text normalization and feature extraction...\")\n",
    "df[\"text_norm\"] = df[text_col].apply(norm)\n",
    "df[\"hashtags\"] = df[\"text_norm\"].apply(extract_hashtags)\n",
    "df[\"emoji_count\"] = df[\"text_norm\"].apply(count_emojis)\n",
    "\n",
    "if ADD_LANG:\n",
    "    print(\"[INFO] Detecting language (this may take a few minutes)...\")\n",
    "    df[\"lang\"] = df[\"text_norm\"].apply(detect_lang_safe)\n",
    "\n",
    "# Time features\n",
    "print(\"[INFO] Adding time-based features...\")\n",
    "df[tcol] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True)\n",
    "n_dropped = df[df[tcol].isna()].shape[0]\n",
    "df = df.dropna(subset=[tcol])\n",
    "print(f\"[INFO] Dropped {n_dropped:,} rows with invalid timestamps\")\n",
    "\n",
    "df[\"date\"] = df[tcol].dt.date\n",
    "df[\"hour\"] = df[tcol].dt.hour\n",
    "df[\"day_of_week\"] = df[tcol].dt.dayofweek\n",
    "df[\"week_start\"] = df[tcol].dt.to_period(\"W-MON\").apply(lambda p: p.start_time.date())\n",
    "\n",
    "# Save enriched data\n",
    "print(f\"[INFO] Saving enriched data to {OUTPUT}\")\n",
    "Path(OUTPUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_parquet(OUTPUT, engine=\"pyarrow\", index=False)\n",
    "\n",
    "print(f\"[OK] Enrichment complete → {OUTPUT} | Final shape: {df.shape}\")\n",
    "\n",
    "# Quick preview\n",
    "display(df[[\"text_norm\", \"hashtags\", \"emoji_count\", \"lang\", \"date\", \"hour\"]].head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8128629,
     "sourceId": 12851871,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18280.832256,
   "end_time": "2025-08-25T10:27:29.189380",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-25T05:22:48.357124",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
