{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e094b833",
   "metadata": {
    "_cell_guid": "90fcd94c-1362-4ed5-81d8-f00860a24ac4",
    "_uuid": "ee69cada-38c9-4589-87e1-93fd5d5f3fea",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-28T05:29:41.166267Z",
     "iopub.status.busy": "2025-08-28T05:29:41.165915Z",
     "iopub.status.idle": "2025-08-28T08:56:06.417675Z",
     "shell.execute_reply": "2025-08-28T08:56:06.416223Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 12385.260798,
     "end_time": "2025-08-28T08:56:06.420310",
     "exception": false,
     "start_time": "2025-08-28T05:29:41.159512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Core Data Files ---\n",
      "Loaded Fusion Results: (39938, 13)\n",
      "Loaded Videos Meta (92759, 15)\n",
      "Loaded Comments Enriched: (4725012, 17)\n",
      "\n",
      "--- Loading New Signal Data Files ---\n",
      "Loaded Amazon Ratings: (2023070, 4)\n",
      "Loaded Top Beauty Products 2024: (15000, 14)\n",
      "Loaded Supply Chain Analysis: (100, 24)\n",
      "\n",
      "--- Merging Core Datasets ---\n",
      "Merged Fusion Results with Videos: (39938, 27)\n",
      "Warning: Expected column not found in comments for aggregation: 'text'. Skipping comment merge.\n",
      "Final Merged Core Dataset Shape: (39938, 27)\n",
      "\n",
      "--- Processing Core Data for Trends ---\n",
      "Creating combined text field for analysis...\n",
      "Cleaning text data...\n",
      "Detecting trending tags...\n",
      "Identifying product gaps...\n",
      "Analyzing trending ingredients...\n",
      "\n",
      "--- Processing New Signal Data ---\n",
      "Analyzing Amazon Ratings data...\n",
      "  -> Identified 50 popular products on Amazon (based on ratings).\n",
      "Analyzing Top Beauty Products 2024 data...\n",
      "  -> Top Categories: ['Serum', 'Mascara', 'Face Oil', 'Highlighter', 'Face Mask']...\n",
      "  -> Top Brands: ['Milk Makeup', 'Make Up For Ever', 'Kiehl’s', 'NARS', 'E.l.f.']...\n",
      "  -> Successful Products (High Rating & Reviews): 30 found.\n",
      "Analyzing Supply Chain data...\n",
      "  -> Top Performing Supply Chain Product Types: ['skincare', 'cosmetics', 'haircare']\n",
      "\n",
      "--- Saving Intermediate Analysis Data ---\n",
      "Saved 'trending_tags.csv' (100 tags)\n",
      "Saved 'product_gaps.csv' (98 gaps)\n",
      "Saved 'trending_ingredients.csv' (31 ingredients)\n",
      "Saved 'top_amazon_products.csv' (50 products)\n",
      "Saved 'top_categories.csv' (10 categories)\n",
      "Saved 'top_brands.csv' (10 brands)\n",
      "Saved 'successful_products.csv' (30 products)\n",
      "Saved 'top_supply_types.csv' (3 types)\n",
      "\n",
      "--- Generating Final Product Recommendations ---\n",
      "\n",
      "--- Generating Product Recommendations using DeepSeek-TNG-R1T2-Chimera via OpenRouter ---\n",
      "\n",
      "--- Generating Revenue Forecasts ---\n",
      "--- Building ML Revenue Forecasting Models ---\n",
      "Validating and cleaning training data...\n",
      "Removed 0 rows with missing values\n",
      "Removed 603 outliers from revenue\n",
      "Removed 90 outliers from margin_pct\n",
      "Training revenue forecasting model...\n",
      "Revenue Model Performance:\n",
      "  CV MAE: 4891.52 (+/- 401.30)\n",
      "  CV R²: 0.999 (+/- 0.000)\n",
      "  CV RMSE: 8031.92 (+/- 773.19)\n",
      "  Best Revenue Params: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Training margin forecasting model...\n",
      "Margin Model Performance:\n",
      "  CV MAE: 0.040 (+/- 0.001)\n",
      "  CV R²: 0.295 (+/- 0.017)\n",
      "  CV RMSE: 0.049 (+/- 0.001)\n",
      "  Best Margin Params: {'max_depth': 5, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "\n",
      "Feature Importance Analysis:\n",
      "Revenue Model - Top 5 Features:\n",
      "  rating_review_interaction: 0.647\n",
      "  revenue_per_review: 0.281\n",
      "  log_review_count: 0.036\n",
      "  review_count: 0.035\n",
      "  rating: 0.000\n",
      "Margin Model - Top 5 Features:\n",
      "  brand_category_interaction: 0.928\n",
      "  category_factor: 0.036\n",
      "  revenue_per_review: 0.010\n",
      "  brand_factor: 0.008\n",
      "  rating_review_interaction: 0.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not create market potential bins with qcut: Bin labels must be one fewer than the number of bin edges\n",
      "Successfully forecasted 20 products with ML models\n",
      "\n",
      "--- Analysis Complete! ---\n",
      "Final recommendations with forecasts saved to 'beauty_innovation_recommendations_with_forecasts.csv' (20 products)\n",
      "\n",
      "--- Top 5 Forecasted Products by Revenue Potential ---\n",
      "1. Clean Pulse Fragrance Mist - Fragrance\n",
      ".0f\n",
      ".1f\n",
      "   Break-even: 14.7 months\n",
      "   Investment: Not Recommended\n",
      "   Key Ingredients: Essential Oils, Coconut Water\n",
      "   Target Market: Natural Scent Seekers\n",
      "\n",
      "2. Body Sculpt Pro Serum - Body Care\n",
      ".0f\n",
      ".1f\n",
      "   Break-even: 14.7 months\n",
      "   Investment: Not Recommended\n",
      "   Key Ingredients: Caffeine, Vegan Retinol Alternative\n",
      "   Target Market: 30+ Body Care Market\n",
      "\n",
      "3. Skin-Like Tint Drops - Makeup\n",
      ".0f\n",
      ".1f\n",
      "   Break-even: 15.4 months\n",
      "   Investment: Not Recommended\n",
      "   Key Ingredients: Collagen, Hyaluronic Acid\n",
      "   Target Market: Gen Z & Minimalist Consumers\n",
      "\n",
      "4. Vegan Lip Cloud Liquid Lipstick - Makeup\n",
      ".0f\n",
      ".1f\n",
      "   Break-even: 15.4 months\n",
      "   Investment: Not Recommended\n",
      "   Key Ingredients: Jojoba Oil, Plant Waxes\n",
      "   Target Market: Vegan Beauty Enthusiasts\n",
      "\n",
      "5. Multi-Blush Gel Balm - Makeup\n",
      ".0f\n",
      ".1f\n",
      "   Break-even: 15.4 months\n",
      "   Investment: Not Recommended\n",
      "   Key Ingredients: Natural Fruit Pigments, Vitamin E\n",
      "   Target Market: TikTok Trend Followers\n",
      "\n",
      "\n",
      "--- Summary of All Forecasted Products ---\n",
      "Total Products: 20\n",
      ".0f\n",
      ".0f\n",
      "Average Break-even: 15.1 months\n",
      "Top Investment Recommendations: {'Not Recommended': 20}\n"
     ]
    }
   ],
   "source": [
    "# beauty_innovation_engine.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Configuration ---\n",
    "# Replace <OPENROUTER_API_KEY> with your actual key\n",
    "OPENROUTER_API_KEY = \"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\"\n",
    "# Optional: Add your site URL and name for rankings\n",
    "YOUR_SITE_URL = \"\" # e.g., \"https://yourwebsite.com\"\n",
    "YOUR_SITE_NAME = \"\" # e.g., \"Your Site Name\"\n",
    "# ---------------------\n",
    "\n",
    "# --- File Paths (Update if paths differ) ---\n",
    "FUSION_RESULTS_PATH = '/kaggle/input/fusion-engine/all_signals_combined.csv'\n",
    "VIDEOS_PATH = '/kaggle/input/datathon-loreal/videos.csv'\n",
    "COMMENTS_PATH = '/kaggle/input/data-cleaning/comments_enriched.parquet' # Optional\n",
    "\n",
    "AMAZON_RATINGS_PATH = '/kaggle/input/amazon-ratings/ratings_Beauty.csv'\n",
    "TOP_PRODUCTS_PATH = '/kaggle/input/most-used-beauty-cosmetics-products-in-the-world/most_used_beauty_cosmetics_products_extended.csv'\n",
    "SUPPLY_CHAIN_PATH = '/kaggle/input/supply-chain-analysis/supply_chain_data.csv'\n",
    "# ------------------------------------------\n",
    "\n",
    "def forecast_product_revenue_and_margin(recommendations_df, sales_data_df, supply_chain_df=None):\n",
    "    \"\"\"\n",
    "    Use machine learning to forecast revenue and margin projections for new product recommendations.\n",
    "    Includes professional business metrics that companies need to see.\n",
    "    \"\"\"\n",
    "    print(\"--- Building ML Revenue Forecasting Models ---\")\n",
    "\n",
    "    if sales_data_df.empty:\n",
    "        print(\"No sales data available for forecasting. Using industry averages.\")\n",
    "        return _apply_industry_averages(recommendations_df)\n",
    "\n",
    "    # Prepare training data from existing products\n",
    "    training_data = _prepare_training_data(sales_data_df, supply_chain_df)\n",
    "\n",
    "    if training_data.empty:\n",
    "        print(\"Insufficient training data. Using industry averages.\")\n",
    "        return _apply_industry_averages(recommendations_df)\n",
    "\n",
    "    # Build forecasting models\n",
    "    revenue_model, margin_model = _build_forecasting_models(training_data)\n",
    "\n",
    "    # Apply models to new recommendations\n",
    "    forecasted_df = _apply_forecasting_models(recommendations_df, revenue_model, margin_model, training_data)\n",
    "\n",
    "    # Add professional business metrics\n",
    "    forecasted_df = _add_business_metrics(forecasted_df)\n",
    "\n",
    "    print(f\"Successfully forecasted {len(forecasted_df)} products with ML models\")\n",
    "    return forecasted_df\n",
    "\n",
    "def _prepare_training_data(sales_data_df, supply_chain_df=None):\n",
    "    \"\"\"Prepare training data for ML models from existing sales data.\"\"\"\n",
    "    training_features = []\n",
    "\n",
    "    # Basic product features\n",
    "    if 'Rating' in sales_data_df.columns:\n",
    "        sales_data_df['Rating'] = pd.to_numeric(sales_data_df['Rating'], errors='coerce')\n",
    "    if 'Number_of_Reviews' in sales_data_df.columns:\n",
    "        sales_data_df['Number_of_Reviews'] = pd.to_numeric(sales_data_df['Number_of_Reviews'], errors='coerce')\n",
    "\n",
    "    # Create synthetic revenue and margin data for training\n",
    "    # In a real scenario, this would come from actual sales data\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "\n",
    "    for idx, row in sales_data_df.iterrows():\n",
    "        if pd.isna(row.get('Rating', 0)) or pd.isna(row.get('Number_of_Reviews', 0)):\n",
    "            continue\n",
    "\n",
    "        # Estimate revenue based on rating and review volume\n",
    "        base_revenue = row['Rating'] * row['Number_of_Reviews'] * np.random.uniform(10, 50)\n",
    "\n",
    "        # Estimate margin based on category and brand strength\n",
    "        category_factor = 1.0\n",
    "        if 'Category' in row and pd.notna(row['Category']):\n",
    "            category_factor = len(str(row['Category'])) / 20  # Simple category complexity factor\n",
    "\n",
    "        brand_factor = 1.0\n",
    "        if 'Brand' in row and pd.notna(row['Brand']):\n",
    "            brand_factor = len(str(row['Brand'])) / 15  # Brand name length as proxy for brand strength\n",
    "\n",
    "        margin_pct = 0.3 + (category_factor * 0.2) + (brand_factor * 0.1) + np.random.normal(0, 0.05)\n",
    "        margin_pct = np.clip(margin_pct, 0.15, 0.65)  # Realistic margin range\n",
    "\n",
    "        feature_dict = {\n",
    "            'rating': row['Rating'],\n",
    "            'review_count': row['Number_of_Reviews'],\n",
    "            'category_factor': category_factor,\n",
    "            'brand_factor': brand_factor,\n",
    "            'revenue': base_revenue,\n",
    "            'margin_pct': margin_pct,\n",
    "            'profit': base_revenue * margin_pct\n",
    "        }\n",
    "\n",
    "        training_features.append(feature_dict)\n",
    "\n",
    "    return pd.DataFrame(training_features)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _build_forecasting_models(training_data):\n",
    "    \"\"\"Build and evaluate ML models for revenue and margin forecasting with comprehensive improvements.\"\"\"\n",
    "    if len(training_data) < 10:\n",
    "        print(\"Insufficient training data for ML models\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # Data validation and cleaning\n",
    "        print(\"Validating and cleaning training data...\")\n",
    "        training_data = training_data.copy()\n",
    "        \n",
    "        # Remove rows with missing values in key columns\n",
    "        key_columns = ['rating', 'review_count', 'category_factor', 'brand_factor', 'revenue', 'margin_pct']\n",
    "        initial_rows = len(training_data)\n",
    "        training_data = training_data.dropna(subset=key_columns)\n",
    "        print(f\"Removed {initial_rows - len(training_data)} rows with missing values\")\n",
    "        \n",
    "        if len(training_data) < 5:\n",
    "            print(\"Insufficient clean training data after validation\")\n",
    "            return None, None\n",
    "\n",
    "        # Feature engineering: Add interaction and log-transformed features\n",
    "        training_data['rating_review_interaction'] = training_data['rating'] * training_data['review_count']\n",
    "        training_data['log_review_count'] = np.log1p(training_data['review_count'])  # log(1 + x) to handle zeros\n",
    "        training_data['revenue_per_review'] = training_data['revenue'] / (training_data['review_count'] + 1)  # Avoid division by zero\n",
    "        training_data['brand_category_interaction'] = training_data['brand_factor'] * training_data['category_factor']\n",
    "\n",
    "        # Remove outliers using IQR method for revenue and margin\n",
    "        for col in ['revenue', 'margin_pct']:\n",
    "            Q1 = training_data[col].quantile(0.25)\n",
    "            Q3 = training_data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers_before = len(training_data)\n",
    "            training_data = training_data[(training_data[col] >= lower_bound) & (training_data[col] <= upper_bound)]\n",
    "            print(f\"Removed {outliers_before - len(training_data)} outliers from {col}\")\n",
    "\n",
    "        feature_cols = ['rating', 'review_count', 'category_factor', 'brand_factor', \n",
    "                        'rating_review_interaction', 'log_review_count', 'revenue_per_review', \n",
    "                        'brand_category_interaction']\n",
    "\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(training_data[feature_cols]), \n",
    "            columns=feature_cols, \n",
    "            index=training_data.index\n",
    "        )\n",
    "\n",
    "        # Enhanced hyperparameter tuning for Random Forest\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200, 300],\n",
    "            'max_depth': [None, 5, 10, 15, 20],\n",
    "            'min_samples_split': [2, 5, 10, 20],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "\n",
    "        # Revenue model with comprehensive evaluation\n",
    "        print(\"Training revenue forecasting model...\")\n",
    "        y_revenue = training_data['revenue']\n",
    "\n",
    "        revenue_model = RandomForestRegressor(random_state=42)\n",
    "        grid_search_rev = GridSearchCV(\n",
    "            revenue_model, \n",
    "            param_grid, \n",
    "            cv=min(5, len(training_data)//2),  # Adjust CV folds based on data size\n",
    "            scoring='neg_mean_absolute_error', \n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid_search_rev.fit(X_scaled, y_revenue)\n",
    "        best_rev_model = grid_search_rev.best_estimator_\n",
    "\n",
    "        # Comprehensive evaluation for revenue model\n",
    "        cv_scores_rev_mae = cross_val_score(best_rev_model, X_scaled, y_revenue, cv=5, scoring='neg_mean_absolute_error')\n",
    "        cv_scores_rev_r2 = cross_val_score(best_rev_model, X_scaled, y_revenue, cv=5, scoring='r2')\n",
    "        cv_scores_rev_rmse = cross_val_score(best_rev_model, X_scaled, y_revenue, cv=5, scoring='neg_root_mean_squared_error')\n",
    "        \n",
    "        print(f\"Revenue Model Performance:\")\n",
    "        print(f\"  CV MAE: {-cv_scores_rev_mae.mean():.2f} (+/- {cv_scores_rev_mae.std() * 2:.2f})\")\n",
    "        print(f\"  CV R²: {cv_scores_rev_r2.mean():.3f} (+/- {cv_scores_rev_r2.std() * 2:.3f})\")\n",
    "        print(f\"  CV RMSE: {-cv_scores_rev_rmse.mean():.2f} (+/- {cv_scores_rev_rmse.std() * 2:.2f})\")\n",
    "        print(f\"  Best Revenue Params: {grid_search_rev.best_params_}\")\n",
    "\n",
    "        # Margin model with comprehensive evaluation\n",
    "        print(\"Training margin forecasting model...\")\n",
    "        y_margin = training_data['margin_pct']\n",
    "\n",
    "        margin_model = RandomForestRegressor(random_state=42)\n",
    "        grid_search_mar = GridSearchCV(\n",
    "            margin_model, \n",
    "            param_grid, \n",
    "            cv=min(5, len(training_data)//2),\n",
    "            scoring='neg_mean_absolute_error', \n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid_search_mar.fit(X_scaled, y_margin)\n",
    "        best_mar_model = grid_search_mar.best_estimator_\n",
    "\n",
    "        # Comprehensive evaluation for margin model\n",
    "        cv_scores_mar_mae = cross_val_score(best_mar_model, X_scaled, y_margin, cv=5, scoring='neg_mean_absolute_error')\n",
    "        cv_scores_mar_r2 = cross_val_score(best_mar_model, X_scaled, y_margin, cv=5, scoring='r2')\n",
    "        cv_scores_mar_rmse = cross_val_score(best_mar_model, X_scaled, y_margin, cv=5, scoring='neg_root_mean_squared_error')\n",
    "        \n",
    "        print(f\"Margin Model Performance:\")\n",
    "        print(f\"  CV MAE: {-cv_scores_mar_mae.mean():.3f} (+/- {cv_scores_mar_mae.std() * 2:.3f})\")\n",
    "        print(f\"  CV R²: {cv_scores_mar_r2.mean():.3f} (+/- {cv_scores_mar_r2.std() * 2:.3f})\")\n",
    "        print(f\"  CV RMSE: {-cv_scores_mar_rmse.mean():.3f} (+/- {cv_scores_mar_rmse.std() * 2:.3f})\")\n",
    "        print(f\"  Best Margin Params: {grid_search_mar.best_params_}\")\n",
    "\n",
    "        # Feature importance analysis\n",
    "        print(\"\\nFeature Importance Analysis:\")\n",
    "        rev_feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': best_rev_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        mar_feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': best_mar_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"Revenue Model - Top 5 Features:\")\n",
    "        for i, row in rev_feature_importance.head().iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.3f}\")\n",
    "            \n",
    "        print(\"Margin Model - Top 5 Features:\")\n",
    "        for i, row in mar_feature_importance.head().iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "        # Store scaler with models for later use\n",
    "        best_rev_model.scaler = scaler\n",
    "        best_mar_model.scaler = scaler\n",
    "        best_rev_model.feature_cols = feature_cols\n",
    "        best_mar_model.feature_cols = feature_cols\n",
    "\n",
    "        return best_rev_model, best_mar_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in model training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def _apply_forecasting_models(recommendations_df, revenue_model, margin_model, training_data):\n",
    "    \"\"\"Apply trained models to forecast new product performance with improved scaling and error handling.\"\"\"\n",
    "    if revenue_model is None or margin_model is None:\n",
    "        return _apply_industry_averages(recommendations_df)\n",
    "\n",
    "    forecasted_products = []\n",
    "\n",
    "    try:\n",
    "        for idx, product in recommendations_df.iterrows():\n",
    "            try:\n",
    "                # Create feature vector for new product\n",
    "                features = _extract_product_features(product, training_data)\n",
    "                \n",
    "                # Scale features using the stored scaler\n",
    "                if hasattr(revenue_model, 'scaler') and hasattr(revenue_model, 'feature_cols'):\n",
    "                    # Create DataFrame with proper column names for scaling\n",
    "                    features_df = pd.DataFrame([features], columns=revenue_model.feature_cols)\n",
    "                    features_scaled = revenue_model.scaler.transform(features_df)\n",
    "                    \n",
    "                    # Forecast revenue\n",
    "                    predicted_revenue = revenue_model.predict(features_scaled)[0]\n",
    "                    \n",
    "                    # Forecast margin\n",
    "                    predicted_margin = margin_model.predict(features_scaled)[0]\n",
    "                else:\n",
    "                    # Fallback to unscaled prediction if scaler is not available\n",
    "                    print(\"Warning: Using unscaled features for prediction\")\n",
    "                    predicted_revenue = revenue_model.predict([features])[0]\n",
    "                    predicted_margin = margin_model.predict([features])[0]\n",
    "\n",
    "                # Ensure predictions are within reasonable bounds\n",
    "                predicted_revenue = max(1000, min(500000, predicted_revenue))  # Between $1K-$500K\n",
    "                predicted_margin = max(0.05, min(0.8, predicted_margin))  # Between 5%-80%\n",
    "\n",
    "                # Calculate profit\n",
    "                predicted_profit = predicted_revenue * predicted_margin\n",
    "\n",
    "                # Determine confidence based on prediction stability\n",
    "                confidence = 'High'\n",
    "                if not training_data.empty:\n",
    "                    revenue_median = training_data['revenue'].median()\n",
    "                    if predicted_revenue < revenue_median * 0.5:\n",
    "                        confidence = 'Low'\n",
    "                    elif predicted_revenue < revenue_median:\n",
    "                        confidence = 'Medium'\n",
    "\n",
    "                # Add forecasting results to product\n",
    "                product_dict = product.to_dict()\n",
    "                product_dict.update({\n",
    "                    'forecasted_yearly_revenue': predicted_revenue,\n",
    "                    'forecasted_margin_pct': predicted_margin,\n",
    "                    'forecasted_yearly_profit': predicted_profit,\n",
    "                    'forecast_confidence': confidence,\n",
    "                    'model_version': 'Enhanced_ML_v2.0'\n",
    "                })\n",
    "\n",
    "                forecasted_products.append(product_dict)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error forecasting product {product.get('product_name', 'Unknown')}: {e}\")\n",
    "                # Use industry averages for this product\n",
    "                product_dict = product.to_dict()\n",
    "                product_dict.update({\n",
    "                    'forecasted_yearly_revenue': 50000 * np.random.uniform(0.8, 1.2),\n",
    "                    'forecasted_margin_pct': 0.35 * np.random.uniform(0.9, 1.1),\n",
    "                    'forecasted_yearly_profit': 17500 * np.random.uniform(0.8, 1.2),\n",
    "                    'forecast_confidence': 'Low (Fallback)',\n",
    "                    'model_version': 'Fallback_Industry_Average'\n",
    "                })\n",
    "                forecasted_products.append(product_dict)\n",
    "\n",
    "        return pd.DataFrame(forecasted_products)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in forecasting process: {e}\")\n",
    "        return _apply_industry_averages(recommendations_df)\n",
    "\n",
    "def _extract_product_features(product, training_data):\n",
    "    \"\"\"Extract features from product recommendation for ML prediction with enhanced feature engineering.\"\"\"\n",
    "    # Default values based on industry standards for new products\n",
    "    rating = 4.2  # Assumed rating for new products\n",
    "    review_count = 100  # Assumed initial reviews\n",
    "\n",
    "    # Category factor based on product category\n",
    "    category_factor = 1.0\n",
    "    if 'category' in product and pd.notna(product['category']):\n",
    "        category_factor = len(str(product['category'])) / 20\n",
    "\n",
    "    # Brand factor (new products get average brand factor)\n",
    "    brand_factor = training_data['brand_factor'].mean() if not training_data.empty else 1.0\n",
    "\n",
    "    # Calculate engineered features (same as in training)\n",
    "    rating_review_interaction = rating * review_count\n",
    "    log_review_count = np.log1p(review_count)\n",
    "    revenue_per_review = 500  # Estimated revenue per review for new products\n",
    "    brand_category_interaction = brand_factor * category_factor\n",
    "\n",
    "    # Return all features in the same order as training\n",
    "    features = [\n",
    "        rating, \n",
    "        review_count, \n",
    "        category_factor, \n",
    "        brand_factor,\n",
    "        rating_review_interaction,\n",
    "        log_review_count,\n",
    "        revenue_per_review,\n",
    "        brand_category_interaction\n",
    "    ]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def _apply_industry_averages(recommendations_df):\n",
    "    \"\"\"Apply industry average projections when ML models can't be built.\"\"\"\n",
    "    print(\"Applying industry average projections...\")\n",
    "\n",
    "    # Beauty industry averages (based on typical market data)\n",
    "    avg_revenue_per_product = 50000  # $50K annual revenue per product\n",
    "    avg_margin_pct = 0.35  # 35% margin\n",
    "    avg_profit_per_product = avg_revenue_per_product * avg_margin_pct\n",
    "\n",
    "    forecasted_products = []\n",
    "\n",
    "    for idx, product in recommendations_df.iterrows():\n",
    "        product_dict = product.to_dict()\n",
    "        product_dict.update({\n",
    "            'forecasted_yearly_revenue': avg_revenue_per_product * np.random.uniform(0.8, 1.2),\n",
    "            'forecasted_margin_pct': avg_margin_pct * np.random.uniform(0.9, 1.1),\n",
    "            'forecasted_yearly_profit': avg_profit_per_product * np.random.uniform(0.8, 1.2),\n",
    "            'forecast_confidence': 'Low (Industry Average)'\n",
    "        })\n",
    "\n",
    "        forecasted_products.append(product_dict)\n",
    "\n",
    "    return pd.DataFrame(forecasted_products)\n",
    "\n",
    "def _add_business_metrics(forecasted_df):\n",
    "    \"\"\"Add professional business metrics that companies need to see.\"\"\"\n",
    "\n",
    "    # Calculate additional business metrics\n",
    "    forecasted_df['forecasted_monthly_revenue'] = forecasted_df['forecasted_yearly_revenue'] / 12\n",
    "    forecasted_df['forecasted_monthly_profit'] = forecasted_df['forecasted_yearly_profit'] / 12\n",
    "\n",
    "    # Break-even analysis (assuming $10K fixed cost per product launch)\n",
    "    fixed_cost_per_product = 10000\n",
    "    forecasted_df['break_even_months'] = fixed_cost_per_product / forecasted_df['forecasted_monthly_profit']\n",
    "    forecasted_df['break_even_months'] = forecasted_df['break_even_months'].clip(1, 24)  # Cap at 2 years\n",
    "\n",
    "    # ROI calculation (Return on Investment)\n",
    "    forecasted_df['roi_pct'] = (forecasted_df['forecasted_yearly_profit'] / fixed_cost_per_product) * 100\n",
    "\n",
    "    # Market potential scoring\n",
    "    try:\n",
    "        forecasted_df['market_potential_score'] = pd.qcut(\n",
    "            forecasted_df['forecasted_yearly_revenue'], 3, labels=['Low', 'Medium', 'High'], duplicates='drop'\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        # Fallback if qcut still fails - use simple percentile-based approach\n",
    "        print(f\"Warning: Could not create market potential bins with qcut: {e}\")\n",
    "        revenue_median = forecasted_df['forecasted_yearly_revenue'].median()\n",
    "        revenue_75th = forecasted_df['forecasted_yearly_revenue'].quantile(0.75)\n",
    "        \n",
    "        conditions = [\n",
    "            forecasted_df['forecasted_yearly_revenue'] >= revenue_75th,\n",
    "            forecasted_df['forecasted_yearly_revenue'] >= revenue_median,\n",
    "            forecasted_df['forecasted_yearly_revenue'] < revenue_median\n",
    "        ]\n",
    "        choices = ['High', 'Medium', 'Low']\n",
    "        forecasted_df['market_potential_score'] = np.select(conditions, choices, default='Low')\n",
    "\n",
    "    # Customer Acquisition Cost estimation (rough estimate)\n",
    "    forecasted_df['estimated_cac'] = forecasted_df['forecasted_yearly_revenue'] * 0.15  # 15% of revenue\n",
    "\n",
    "    # Customer Lifetime Value\n",
    "    forecasted_df['estimated_clv'] = forecasted_df['forecasted_yearly_revenue'] * 2.5  # 2.5x annual revenue\n",
    "\n",
    "    # Profitability index\n",
    "    forecasted_df['profitability_index'] = forecasted_df['forecasted_margin_pct'] * forecasted_df['roi_pct'] / 100\n",
    "\n",
    "    # Risk assessment\n",
    "    forecasted_df['risk_level'] = pd.cut(\n",
    "        forecasted_df['break_even_months'],\n",
    "        bins=[0, 6, 12, float('inf')],\n",
    "        labels=['Low Risk', 'Medium Risk', 'High Risk']\n",
    "    )\n",
    "\n",
    "    # Investment recommendation\n",
    "    conditions = [\n",
    "        (forecasted_df['profitability_index'] > 1.5) & (forecasted_df['risk_level'] == 'Low Risk'),\n",
    "        (forecasted_df['profitability_index'] > 1.0) & (forecasted_df['risk_level'].isin(['Low Risk', 'Medium Risk'])),\n",
    "        (forecasted_df['profitability_index'] > 0.5)\n",
    "    ]\n",
    "    choices = ['Strong Recommend', 'Recommend', 'Consider']\n",
    "    forecasted_df['investment_recommendation'] = np.select(conditions, choices, default='Not Recommended')\n",
    "\n",
    "    return forecasted_df\n",
    "\n",
    "# ------------------------------------------\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and merge all required data files\"\"\"\n",
    "    print(\"--- Loading Core Data Files ---\")\n",
    "    dataframes = {}\n",
    "\n",
    "    # --- Core Data Files ---\n",
    "    try:\n",
    "        dataframes['fusion_results'] = pd.read_csv(FUSION_RESULTS_PATH)\n",
    "        print(f\"Loaded Fusion Results: {dataframes['fusion_results'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {FUSION_RESULTS_PATH}\")\n",
    "        dataframes['fusion_results'] = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        dataframes['videos_df'] = pd.read_csv(VIDEOS_PATH)\n",
    "        print(f\"Loaded Videos Meta {dataframes['videos_df'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {VIDEOS_PATH}\")\n",
    "        dataframes['videos_df'] = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        dataframes['comments_df'] = pd.read_parquet(COMMENTS_PATH)\n",
    "        print(f\"Loaded Comments Enriched: {dataframes['comments_df'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not find {COMMENTS_PATH}. Proceeding without comments data.\")\n",
    "        dataframes['comments_df'] = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error loading comments: {e}. Proceeding without comments data.\")\n",
    "        dataframes['comments_df'] = pd.DataFrame()\n",
    "\n",
    "    # --- New Signal Data Files ---\n",
    "    print(\"\\n--- Loading New Signal Data Files ---\")\n",
    "    try:\n",
    "        dataframes['amazon_ratings'] = pd.read_csv(AMAZON_RATINGS_PATH)\n",
    "        print(f\"Loaded Amazon Ratings: {dataframes['amazon_ratings'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not find {AMAZON_RATINGS_PATH}. Analysis will proceed without it.\")\n",
    "        dataframes['amazon_ratings'] = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error loading Amazon Ratings: {e}. Proceeding without it.\")\n",
    "        dataframes['amazon_ratings'] = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        dataframes['top_products'] = pd.read_csv(TOP_PRODUCTS_PATH)\n",
    "        print(f\"Loaded Top Beauty Products 2024: {dataframes['top_products'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not find {TOP_PRODUCTS_PATH}. Analysis will proceed without it.\")\n",
    "        dataframes['top_products'] = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error loading Top Products: {e}. Proceeding without it.\")\n",
    "        dataframes['top_products'] = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        dataframes['supply_chain'] = pd.read_csv(SUPPLY_CHAIN_PATH)\n",
    "        print(f\"Loaded Supply Chain Analysis: {dataframes['supply_chain'].shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not find {SUPPLY_CHAIN_PATH}. Analysis will proceed without it.\")\n",
    "        dataframes['supply_chain'] = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error loading Supply Chain: {e}. Proceeding without it.\")\n",
    "        dataframes['supply_chain'] = pd.DataFrame()\n",
    "\n",
    "    # --- Merge Core Datasets ---\n",
    "    print(\"\\n--- Merging Core Datasets ---\")\n",
    "    merged_df = pd.DataFrame()\n",
    "    if not dataframes['fusion_results'].empty and not dataframes['videos_df'].empty:\n",
    "        merged_df = dataframes['fusion_results'].merge(dataframes['videos_df'], on='videoId', how='left')\n",
    "        print(f\"Merged Fusion Results with Videos: {merged_df.shape}\")\n",
    "\n",
    "        if not dataframes['comments_df'].empty:\n",
    "            try:\n",
    "                # Aggregate comments per video\n",
    "                dataframes['comments_df']['text'] = dataframes['comments_df']['text'].fillna('')\n",
    "                comments_agg = dataframes['comments_df'].groupby('videoId').agg({\n",
    "                    'text': lambda x: ' '.join(x.astype(str)),\n",
    "                    'likeCount': 'sum',\n",
    "                    # Assuming 'authorDisplayName' count represents comment count if no specific count column\n",
    "                    'authorDisplayName': 'count'\n",
    "                }).reset_index()\n",
    "                comments_agg.rename(columns={'text': 'all_comments', 'authorDisplayName': 'comment_count'}, inplace=True)\n",
    "                merged_df = merged_df.merge(comments_agg, on='videoId', how='left')\n",
    "                print(f\"Merged with Aggregated Comments: {merged_df.shape}\")\n",
    "            except KeyError as e:\n",
    "                print(f\"Warning: Expected column not found in comments for aggregation: {e}. Skipping comment merge.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error aggregating/merging comments: {e}. Skipping comment merge.\")\n",
    "    else:\n",
    "        print(\"Warning: Insufficient core data (fusion_results or videos_df) to perform merge.\")\n",
    "        # Return individual dataframes if core merge fails\n",
    "        return dataframes\n",
    "\n",
    "    print(f\"Final Merged Core Dataset Shape: {merged_df.shape}\")\n",
    "    dataframes['merged_core'] = merged_df\n",
    "    return dataframes\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters and digits (keep spaces and letters)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Optional: Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def extract_beauty_terms(text):\n",
    "    \"\"\"Extract beauty-related terms from text\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return []\n",
    "    beauty_keywords = [\n",
    "        'skincare', 'moisturizer', 'serum', 'cream', 'lotion', 'toner', 'cleanser', 'exfoliant',\n",
    "        'makeup', 'foundation', 'concealer', 'blush', 'eyeshadow', 'lipstick', 'mascara', 'eyeliner',\n",
    "        'haircare', 'shampoo', 'conditioner', 'treatment', 'mask', 'oil',\n",
    "        'fragrance', 'perfume', 'cologne', 'scent',\n",
    "        'ingredient', 'vitamin', 'acid', 'retinol', 'hyaluronic', 'niacinamide', 'salicylic',\n",
    "        'natural', 'organic', 'vegan', 'cruelty-free', 'sustainable',\n",
    "        'trend', 'viral', 'popular', 'best', 'new', 'innovative',\n",
    "        'skin', 'hair', 'beauty', 'cosmetic', 'product'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    found_terms = [keyword for keyword in beauty_keywords if re.search(r'\\b' + re.escape(keyword) + r'\\b', text_lower)]\n",
    "    return found_terms\n",
    "\n",
    "# --- Functions for Gap Analysis ---\n",
    "def extract_product_mentions(text):\n",
    "    \"\"\"Extract product mentions from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text_lower = text.lower()\n",
    "    product_patterns = [\n",
    "        r'\\b(\\w+[-\\s])*(serum|cream|lotion|moisturizer|mask|treatment|oil|gel)\\b',\n",
    "        r'\\b(\\w+[-\\s])*(foundation|concealer|blush|bronzer|highlighter)\\b',\n",
    "        r'\\b(\\w+[-\\s])*(eyeshadow|eyeliner|mascara|lipstick|lip gloss)\\b',\n",
    "        r'\\b(\\w+[-\\s])*(shampoo|conditioner|hair mask|hair oil)\\b',\n",
    "        r'\\b(\\w+[-\\s])*(perfume|cologne|body lotion|body wash)\\b'\n",
    "    ]\n",
    "    products = []\n",
    "    for pattern in product_patterns:\n",
    "        matches = re.findall(pattern, text_lower)\n",
    "        for match in matches:\n",
    "            if isinstance(match, tuple):\n",
    "                product_parts = [m.strip() for m in match if m.strip()]\n",
    "                if product_parts:\n",
    "                    product = ' '.join(product_parts)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                product = match.strip()\n",
    "            if product and len(product) > 2 and product not in ['the', 'and', 'for']:\n",
    "                 products.append(product)\n",
    "    return products\n",
    "\n",
    "def extract_ingredients(text):\n",
    "    \"\"\"Extract ingredient mentions from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text_lower = text.lower()\n",
    "    ingredients = [\n",
    "        r'\\bhyaluronic acid\\b', r'\\bvitamin c\\b', r'\\bvitamin e\\b', r'\\bretinol\\b',\n",
    "        r'\\bniacinamide\\b', r'\\bsalicylic acid\\b', r'\\bglycolic acid\\b', r'\\blactic acid\\b',\n",
    "        r'\\bazelaic acid\\b', r'\\bceramide\\b', r'\\bcollagen\\b', r'\\bpeptides\\b',\n",
    "        r'\\bsnail mucin\\b', r'\\bcharcoal\\b', r'\\btea tree oil\\b', r'\\brosehip oil\\b',\n",
    "        r'\\bargan oil\\b', r'\\bjojoba oil\\b', r'\\bshea butter\\b', r'\\baloe vera\\b',\n",
    "        r'\\bwitch hazel\\b', r'\\bgreen tea\\b', r'\\bcentella asiatica\\b',\n",
    "        r'\\btranexamic acid\\b', r'\\bkojic acid\\b', r'\\bmandelic acid\\b',\n",
    "        r'\\bnatural\\b', r'\\borganic\\b', r'\\bvegan\\b', r'\\bcruelty[-\\s]free\\b'\n",
    "    ]\n",
    "    found_ingredients = []\n",
    "    for ingredient_pattern in ingredients:\n",
    "        matches = re.findall(ingredient_pattern, text_lower)\n",
    "        found_ingredients.extend(matches)\n",
    "    return found_ingredients\n",
    "# --- End Gap Analysis Functions ---\n",
    "\n",
    "def detect_trending_tags(df, top_n=100):\n",
    "    \"\"\"Detect trending tags from the dataset\"\"\"\n",
    "    all_tags = []\n",
    "    if 'tags' in df.columns:\n",
    "        for tags in df['tags'].dropna():\n",
    "            try:\n",
    "                if isinstance(tags, str) and tags.startswith('[') and tags.endswith(']'):\n",
    "                    tag_list = ast.literal_eval(tags)\n",
    "                    if isinstance(tag_list, list):\n",
    "                        all_tags.extend([tag.strip().lower() for tag in tag_list if tag.strip()])\n",
    "                else:\n",
    "                    delimiters = [',', '|', ';']\n",
    "                    delimiter_found = False\n",
    "                    for delim in delimiters:\n",
    "                        if delim in str(tags):\n",
    "                            tag_list = str(tags).split(delim)\n",
    "                            all_tags.extend([tag.strip().lower() for tag in tag_list if tag.strip()])\n",
    "                            delimiter_found = True\n",
    "                            break\n",
    "                    if not delimiter_found:\n",
    "                         all_tags.append(str(tags).strip().lower())\n",
    "            except (ValueError, SyntaxError):\n",
    "                all_tags.append(str(tags).strip().lower())\n",
    "\n",
    "    text_fields = []\n",
    "    for idx, row in df.iterrows():\n",
    "        combined_text = ''\n",
    "        if 'title' in df.columns and pd.notna(row['title']):\n",
    "            combined_text += str(row['title']) + ' '\n",
    "        if 'description' in df.columns and pd.notna(row['description']):\n",
    "            combined_text += str(row['description']) + ' '\n",
    "        text_fields.append(combined_text.strip())\n",
    "\n",
    "    if text_fields:\n",
    "        try:\n",
    "            tfidf = TfidfVectorizer(max_features=2000, stop_words='english', ngram_range=(1, 2), max_df=0.95, min_df=2)\n",
    "            tfidf_matrix = tfidf.fit_transform(text_fields)\n",
    "            feature_names = tfidf.get_feature_names_out()\n",
    "            tfidf_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "            tag_counts = Counter(all_tags)\n",
    "            tag_scores = {tag: count for tag, count in tag_counts.items() if tag}\n",
    "            tfidf_weight = 50\n",
    "            for i, score in enumerate(tfidf_scores):\n",
    "                if score > 0 and feature_names[i]:\n",
    "                    tag_scores[feature_names[i]] = tag_scores.get(feature_names[i], 0) + score * tfidf_weight\n",
    "            sorted_tags = sorted(tag_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            return sorted_tags[:top_n]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in TF-IDF processing for tags: {e}. Returning tag counts only.\")\n",
    "            tag_counts = Counter(all_tags)\n",
    "            sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            return sorted_tags[:top_n]\n",
    "    else:\n",
    "        tag_counts = Counter(all_tags)\n",
    "        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_tags[:top_n]\n",
    "\n",
    "def identify_product_gaps(df):\n",
    "    \"\"\"Identify product gaps based on frequently mentioned products\"\"\"\n",
    "    loreal_brands = [\n",
    "        'loreal', 'l\\'oreal', 'lancome', 'kerastase', 'kiehl', 'kahl', 'ysl', 'yves saint laurent',\n",
    "        'giorgio armani', 'armani', 'maybelline', 'nyx', 'essie', 'matrix', 'redken', 'pureology',\n",
    "        'vichy', 'la roche-posay', 'derma tox', 'skin ceuticals', 'urban decay'\n",
    "    ]\n",
    "\n",
    "    if 'combined_text' not in df.columns:\n",
    "        print(\"Creating temporary combined text for product gap analysis...\")\n",
    "        text_fields = []\n",
    "        for idx, row in df.iterrows():\n",
    "            combined_text = ''\n",
    "            if 'title' in df.columns and pd.notna(row['title']):\n",
    "                combined_text += str(row['title']) + ' '\n",
    "            if 'description' in df.columns and pd.notna(row['description']):\n",
    "                combined_text += str(row['description']) + ' '\n",
    "            if 'tags' in df.columns and pd.notna(row['tags']):\n",
    "                 combined_text += str(row['tags']) + ' '\n",
    "            text_fields.append(combined_text.strip())\n",
    "        df_temp_text = pd.Series(text_fields, name='combined_text')\n",
    "    else:\n",
    "        df_temp_text = df['combined_text']\n",
    "\n",
    "    all_products = []\n",
    "    for text in df_temp_text.dropna():\n",
    "        products = extract_product_mentions(text)\n",
    "        all_products.extend(products)\n",
    "\n",
    "    product_counts = Counter(all_products)\n",
    "    potential_gaps = []\n",
    "    for product, count in product_counts.most_common(100):\n",
    "        product_lower = product.lower()\n",
    "        is_loreal = any(brand in product_lower for brand in loreal_brands)\n",
    "        if not is_loreal and count > 5 and len(product) > 3:\n",
    "             potential_gaps.append((product, count))\n",
    "    return potential_gaps\n",
    "\n",
    "def get_trending_ingredients(df):\n",
    "    \"\"\"Extract trending ingredients\"\"\"\n",
    "    if 'combined_text' not in df.columns:\n",
    "        print(\"Creating temporary combined text for ingredient analysis...\")\n",
    "        text_fields = []\n",
    "        for idx, row in df.iterrows():\n",
    "            combined_text = ''\n",
    "            if 'title' in df.columns and pd.notna(row['title']):\n",
    "                combined_text += str(row['title']) + ' '\n",
    "            if 'description' in df.columns and pd.notna(row['description']):\n",
    "                combined_text += str(row['description']) + ' '\n",
    "            if 'tags' in df.columns and pd.notna(row['tags']):\n",
    "                 combined_text += str(row['tags']) + ' '\n",
    "            if 'all_comments' in df.columns and pd.notna(row['all_comments']):\n",
    "                combined_text += str(row['all_comments']) + ' '\n",
    "            text_fields.append(combined_text.strip())\n",
    "        df_temp_text = pd.Series(text_fields, name='combined_text')\n",
    "    else:\n",
    "        df_temp_text = df['combined_text']\n",
    "\n",
    "    all_ingredients = []\n",
    "    for text in df_temp_text.dropna():\n",
    "        ingredients = extract_ingredients(text)\n",
    "        all_ingredients.extend(ingredients)\n",
    "\n",
    "    ingredient_counts = Counter(all_ingredients)\n",
    "    return ingredient_counts.most_common(50)\n",
    "\n",
    "# --- New Signal Processing Functions ---\n",
    "def analyze_amazon_data(df_amazon):\n",
    "    \"\"\"Analyze Amazon ratings data for popular products/brands.\"\"\"\n",
    "    print(\"Analyzing Amazon Ratings data...\")\n",
    "    if df_amazon.empty:\n",
    "        print(\"  -> No Amazon data available.\")\n",
    "        return [], []\n",
    "\n",
    "    try:\n",
    "        # Calculate average rating and review count per ProductId\n",
    "        product_stats = df_amazon.groupby('ProductId').agg(\n",
    "            avg_rating=('Rating', 'mean'),\n",
    "            num_reviews=('Rating', 'count')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Filter for products with sufficient reviews (e.g., > 10) and high average rating (> 4.0)\n",
    "        popular_products = product_stats[\n",
    "            (product_stats['num_reviews'] > 10) &\n",
    "            (product_stats['avg_rating'] > 4.0)\n",
    "        ].nlargest(50, ['avg_rating', 'num_reviews']) # Get top 50\n",
    "\n",
    "        # Placeholder: We don't have product names/brands from ProductId alone easily.\n",
    "        # This would ideally be joined with product metadata.\n",
    "        # For now, just report ProductIds.\n",
    "        top_amazon_products = popular_products['ProductId'].tolist()\n",
    "        print(f\"  -> Identified {len(top_amazon_products)} popular products on Amazon (based on ratings).\")\n",
    "        return top_amazon_products, [] # Return empty list for brands for now\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Error analyzing Amazon  {e}\")\n",
    "        return [], []\n",
    "\n",
    "def analyze_top_products_data(df_top_products):\n",
    "    \"\"\"Analyze the 'Top Beauty Products 2024' list.\"\"\"\n",
    "    print(\"Analyzing Top Beauty Products 2024 data...\")\n",
    "    if df_top_products.empty:\n",
    "        print(\"  -> No Top Products data available.\")\n",
    "        return [], [], []\n",
    "\n",
    "    try:\n",
    "        # 1. Identify top categories\n",
    "        top_categories = df_top_products['Category'].value_counts().head(10).index.tolist()\n",
    "\n",
    "        # 2. Identify top brands (based on presence in list)\n",
    "        top_brands = df_top_products['Brand'].value_counts().head(10).index.tolist()\n",
    "\n",
    "        # 3. Identify products with high ratings and many reviews (success indicators)\n",
    "        # Assuming 'Rating' and 'Number_of_Reviews' are numeric\n",
    "        df_top_products['Rating'] = pd.to_numeric(df_top_products['Rating'], errors='coerce')\n",
    "        df_top_products['Number_of_Reviews'] = pd.to_numeric(df_top_products['Number_of_Reviews'], errors='coerce')\n",
    "\n",
    "        successful_products_df = df_top_products[\n",
    "            (df_top_products['Rating'] > 4.0) &\n",
    "            (df_top_products['Number_of_Reviews'] > 100) # Arbitrary threshold\n",
    "        ].nlargest(30, 'Number_of_Reviews') # Get top 30 by review count\n",
    "\n",
    "        successful_product_names = successful_products_df['Product_Name'].tolist()\n",
    "        successful_brands_from_list = successful_products_df['Brand'].unique().tolist()\n",
    "\n",
    "        print(f\"  -> Top Categories: {top_categories[:5]}...\")\n",
    "        print(f\"  -> Top Brands: {top_brands[:5]}...\")\n",
    "        print(f\"  -> Successful Products (High Rating & Reviews): {len(successful_product_names)} found.\")\n",
    "        return top_categories, top_brands, successful_product_names, successful_brands_from_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Error analyzing Top Products  {e}\")\n",
    "        return [], [], [], []\n",
    "\n",
    "def analyze_supply_chain_data(df_supply_chain):\n",
    "    \"\"\"Analyze supply chain data for high-performing product types.\"\"\"\n",
    "    print(\"Analyzing Supply Chain data...\")\n",
    "    if df_supply_chain.empty:\n",
    "        print(\"  -> No Supply Chain data available.\")\n",
    "        return [], []\n",
    "\n",
    "    try:\n",
    "        # Calculate performance metrics per product type\n",
    "        supply_metrics = df_supply_chain.groupby('Product type').agg(\n",
    "            total_revenue=('Revenue generated', 'sum'),\n",
    "            total_sold=('Number of products sold', 'sum'),\n",
    "            avg_availability=('Availability', 'mean'),\n",
    "            avg_lead_time=('Lead times', 'mean')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Score product types (example: prioritize high revenue, high sales, high availability, low lead time)\n",
    "        # Normalize metrics (simple min-max for this example)\n",
    "        supply_metrics['norm_revenue'] = (supply_metrics['total_revenue'] - supply_metrics['total_revenue'].min()) / (supply_metrics['total_revenue'].max() - supply_metrics['total_revenue'].min() + 1e-8)\n",
    "        supply_metrics['norm_sold'] = (supply_metrics['total_sold'] - supply_metrics['total_sold'].min()) / (supply_metrics['total_sold'].max() - supply_metrics['total_sold'].min() + 1e-8)\n",
    "        supply_metrics['norm_avail'] = (supply_metrics['avg_availability'] - supply_metrics['avg_availability'].min()) / (supply_metrics['avg_availability'].max() - supply_metrics['avg_availability'].min() + 1e-8)\n",
    "        # Invert lead time for scoring (lower is better)\n",
    "        supply_metrics['norm_lead'] = 1 - ((supply_metrics['avg_lead_time'] - supply_metrics['avg_lead_time'].min()) / (supply_metrics['avg_lead_time'].max() - supply_metrics['avg_lead_time'].min() + 1e-8))\n",
    "\n",
    "        # Simple weighted score (weights can be adjusted)\n",
    "        supply_metrics['performance_score'] = (\n",
    "            0.3 * supply_metrics['norm_revenue'] +\n",
    "            0.3 * supply_metrics['norm_sold'] +\n",
    "            0.2 * supply_metrics['norm_avail'] +\n",
    "            0.2 * supply_metrics['norm_lead']\n",
    "        )\n",
    "\n",
    "        # Get top performing product types\n",
    "        top_supply_types = supply_metrics.nlargest(10, 'performance_score')['Product type'].tolist()\n",
    "        print(f\"  -> Top Performing Supply Chain Product Types: {top_supply_types}\")\n",
    "        return top_supply_types\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Error analyzing Supply Chain  {e}\")\n",
    "        return []\n",
    "\n",
    "# --- End New Signal Processing Functions ---\n",
    "\n",
    "def generate_deepseek_recommendations(\n",
    "    product_gaps, trending_ingredients, trending_tags,\n",
    "    top_amazon_products, top_categories, top_brands,\n",
    "    successful_products, top_supply_types, successful_brands_from_list\n",
    "):\n",
    "    \"\"\"Generate product recommendations using OpenRouter DeepSeek API\"\"\"\n",
    "    recommendations = []\n",
    "\n",
    "    # Prepare data for the LLM\n",
    "    product_gap_text = \"\\n\".join([f\"- {product} (mentioned {count} times)\" for product, count in product_gaps[:20]])\n",
    "    ingredient_text = \"\\n\".join([f\"- {ingredient} (mentioned {count} times)\" for ingredient, count in trending_ingredients[:20]])\n",
    "    tag_text = \"\\n\".join([f\"- {tag} (score: {score:.2f})\" for tag, score in trending_tags[:30]])\n",
    "\n",
    "    # New Signals\n",
    "    amazon_text = \"\\n\".join([f\"- Product ID: {pid}\" for pid in top_amazon_products[:20]]) if top_amazon_products else \"No data available.\"\n",
    "    categories_text = \"\\n\".join([f\"- {cat}\" for cat in top_categories[:10]]) if top_categories else \"No data available.\"\n",
    "    brands_text = \"\\n\".join([f\"- {brand}\" for brand in list(set(top_brands + successful_brands_from_list))[:15]]) if top_brands or successful_brands_from_list else \"No data available.\"\n",
    "    successful_products_text = \"\\n\".join([f\"- {prod}\" for prod in successful_products[:20]]) if successful_products else \"No data available.\"\n",
    "    supply_chain_text = \"\\n\".join([f\"- {ptype}\" for ptype in top_supply_types[:10]]) if top_supply_types else \"No data available.\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert beauty industry analyst and innovation strategist for L'Oréal. Your task is to analyze multiple data signals and recommend innovative beauty products that L'Oréal should consider developing to fill market gaps and capitalize on trends.\n",
    "\n",
    "    Based on the following comprehensive beauty trend and market data, recommend 15-20 innovative beauty products that L'Oréal should consider developing. These products should NOT be part of L'Oréal's current portfolio and should leverage the trending concepts and market insights.\n",
    "\n",
    "    --- Trending Data from Social Media/Video Analysis ---\n",
    "    Trending Product Concepts (Market Gaps):\n",
    "    {product_gap_text if product_gap_text else 'No data available.'}\n",
    "\n",
    "    Trending Ingredients:\n",
    "    {ingredient_text if ingredient_text else 'No data available.'}\n",
    "\n",
    "    Trending Tags/Keywords:\n",
    "    {tag_text if tag_text else 'No data available.'}\n",
    "\n",
    "    --- Market Data Signals ---\n",
    "\n",
    "    1. Popular Products on Amazon (High Ratings & Reviews):\n",
    "    {amazon_text}\n",
    "\n",
    "    2. Top Beauty Product Categories (2024 List):\n",
    "    {categories_text}\n",
    "\n",
    "    3. Leading Beauty Brands (2024 List):\n",
    "    {brands_text}\n",
    "\n",
    "    4. Highly Successful Existing Products (High Rating & Many Reviews):\n",
    "    {successful_products_text}\n",
    "\n",
    "    5. Top Performing Product Types in Supply Chain (High Revenue/Sales):\n",
    "    {supply_chain_text}\n",
    "\n",
    "    --- Instructions ---\n",
    "    Please provide your recommendations in this exact format, one product per line:\n",
    "    Product Name|Product Category|Key Ingredients|Target Market|Innovation Description\n",
    "\n",
    "    Example format (do not include this example in your output):\n",
    "    Vitamin C Glow Serum|Skincare|Vitamin C, Hyaluronic Acid|Millennials & Gen Z|A stable vitamin C formulation with time-release technology for consistent brightening\n",
    "\n",
    "    Focus on products that are:\n",
    "    1. Truly innovative and not currently in L'Oréal's portfolio.\n",
    "    2. Based on the trending ingredients, concepts, categories, and successful market examples provided.\n",
    "    3. Address specific consumer needs or market segments indicated by the data.\n",
    "    4. Align with high-performing supply chain categories where possible.\n",
    "    5. Provide a clear innovation description explaining the unique value proposition.\n",
    "    6. Use the exact format specified.\n",
    "\n",
    "    Provide only the list of recommendations, nothing else. Aim for diversity across categories (Skincare, Haircare, Makeup, Fragrance, Body Care) and target markets.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Generating Product Recommendations using DeepSeek-TNG-R1T2-Chimera via OpenRouter ---\")\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        if YOUR_SITE_URL:\n",
    "            headers[\"HTTP-Referer\"] = YOUR_SITE_URL\n",
    "        if YOUR_SITE_NAME:\n",
    "            headers[\"X-Title\"] = YOUR_SITE_NAME\n",
    "\n",
    "        payload = {\n",
    "            \"model\": \"tngtech/deepseek-r1t2-chimera:free\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 2500, # Increased token limit for potentially longer list\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            data=json.dumps(payload),\n",
    "            timeout=180 # Increased timeout\n",
    "        )\n",
    "\n",
    "        response.raise_for_status()\n",
    "        response_data = response.json()\n",
    "\n",
    "        if 'choices' in response_data and len(response_data['choices']) > 0:\n",
    "            generated_text = response_data['choices'][0].get('message', {}).get('content', '')\n",
    "            if generated_text:\n",
    "                lines = generated_text.strip().split('\\n')\n",
    "                for line in lines:\n",
    "                    if '|' in line and not line.startswith(\"---\") and not \"Example format\" in line:\n",
    "                        parts = line.split('|')\n",
    "                        if len(parts) >= 5:\n",
    "                            recommendations.append({\n",
    "                                'product_name': parts[0].strip(),\n",
    "                                'category': parts[1].strip(),\n",
    "                                'key_ingredients': parts[2].strip(),\n",
    "                                'target_market': parts[3].strip(),\n",
    "                                'innovation_description': parts[4].strip()\n",
    "                            })\n",
    "                        else:\n",
    "                            print(f\"Warning: Skipping malformed recommendation line: {line}\")\n",
    "\n",
    "            else:\n",
    "                print(\"Warning: No content found in the API response.\")\n",
    "        else:\n",
    "            print(\"Warning: Unexpected API response structure.\")\n",
    "            print(f\"API Response Sample: {str(response_data)[:500]}...\")\n",
    "\n",
    "        if not recommendations:\n",
    "             print(\"Warning: No valid recommendations parsed from API response. Using fallback recommendations.\")\n",
    "             recommendations = [\n",
    "                {\n",
    "                    'product_name': 'Hyaluronic Acid Overnight Mask',\n",
    "                    'category': 'Skincare',\n",
    "                    'key_ingredients': 'Hyaluronic Acid, Ceramides',\n",
    "                    'target_market': 'All Ages',\n",
    "                    'innovation_description': 'Advanced moisture delivery system for intensive overnight hydration'\n",
    "                }\n",
    "             ]\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error calling OpenRouter API: {e}\")\n",
    "        print(\"Using fallback recommendations.\")\n",
    "        recommendations = [\n",
    "            {\n",
    "                'product_name': 'CBD Soothing Body Lotion',\n",
    "                'category': 'Body Care',\n",
    "                'key_ingredients': 'CBD, Aloe Vera, Chamomile',\n",
    "                'target_market': 'Sensitive Skin',\n",
    "                'innovation_description': 'Calms irritated skin and provides long-lasting hydration'\n",
    "            }\n",
    "        ]\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON response from API: {e}\")\n",
    "        print(\"Using fallback recommendations.\")\n",
    "        recommendations = [\n",
    "            {\n",
    "                'product_name': 'Adaptogenic Stress Relief Cream',\n",
    "                'category': 'Skincare',\n",
    "                'key_ingredients': 'Ashwagandha, Reishi Mushroom',\n",
    "                'target_market': 'Gen Z & Millennials',\n",
    "                'innovation_description': 'Skincare that addresses stress-related skin concerns'\n",
    "            }\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during API call or processing: {e}\")\n",
    "        print(\"Using fallback recommendations.\")\n",
    "        recommendations = [\n",
    "            {\n",
    "                'product_name': 'Multi-Peptide Firming Eye Cream',\n",
    "                'category': 'Skincare',\n",
    "                'key_ingredients': 'Matrixyl, Argireline, Peptides',\n",
    "                'target_market': 'Aging Skin',\n",
    "                'innovation_description': 'Targets multiple signs of aging around the eye area'\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the beauty trend analysis\"\"\"\n",
    "    try:\n",
    "        # --- 1. Load and Merge Data ---\n",
    "        data_dict = load_data()\n",
    "\n",
    "        # --- 2. Process Core Data for Trends ---\n",
    "        df_core = data_dict.get('merged_core', pd.DataFrame())\n",
    "        if df_core.empty:\n",
    "             print(\"\\nError: Failed to create merged core dataset. Cannot proceed with core trend analysis.\")\n",
    "             # We can still try to proceed with other available data signals\n",
    "             df_core = pd.DataFrame() # Ensure it's a DataFrame even if empty\n",
    "        else:\n",
    "            print(\"\\n--- Processing Core Data for Trends ---\")\n",
    "            if 'combined_text' not in df_core.columns:\n",
    "                print(\"Creating combined text field for analysis...\")\n",
    "                text_fields = []\n",
    "                for idx, row in df_core.iterrows():\n",
    "                    combined_text = ''\n",
    "                    if 'title' in df_core.columns and pd.notna(row['title']):\n",
    "                        combined_text += str(row['title']) + ' '\n",
    "                    if 'description' in df_core.columns and pd.notna(row['description']):\n",
    "                        combined_text += str(row['description']) + ' '\n",
    "                    if 'tags' in df_core.columns and pd.notna(row['tags']):\n",
    "                        combined_text += str(row['tags']) + ' '\n",
    "                    if 'all_comments' in df_core.columns and pd.notna(row['all_comments']):\n",
    "                        combined_text += str(row['all_comments']) + ' '\n",
    "                    text_fields.append(combined_text.strip())\n",
    "                df_core['combined_text'] = text_fields\n",
    "            else:\n",
    "                print(\"Using existing 'combined_text' column.\")\n",
    "\n",
    "            print(\"Cleaning text data...\")\n",
    "            df_core['cleaned_text'] = df_core['combined_text'].apply(clean_text)\n",
    "\n",
    "            print(\"Detecting trending tags...\")\n",
    "            trending_tags = detect_trending_tags(df_core)\n",
    "            print(\"Identifying product gaps...\")\n",
    "            product_gaps = identify_product_gaps(df_core)\n",
    "            print(\"Analyzing trending ingredients...\")\n",
    "            trending_ingredients = get_trending_ingredients(df_core)\n",
    "        # Provide empty lists if core analysis failed\n",
    "        if 'trending_tags' not in locals(): trending_tags = []\n",
    "        if 'product_gaps' not in locals(): product_gaps = []\n",
    "        if 'trending_ingredients' not in locals(): trending_ingredients = []\n",
    "\n",
    "\n",
    "        # --- 3. Process New Signal Data ---\n",
    "        print(\"\\n--- Processing New Signal Data ---\")\n",
    "        # a. Amazon Ratings\n",
    "        top_amazon_products, top_amazon_brands = analyze_amazon_data(data_dict.get('amazon_ratings', pd.DataFrame()))\n",
    "\n",
    "        # b. Top Beauty Products 2024\n",
    "        top_categories, top_brands, successful_products, successful_brands_from_list = analyze_top_products_data(data_dict.get('top_products', pd.DataFrame()))\n",
    "\n",
    "        # c. Supply Chain Analysis\n",
    "        top_supply_types = analyze_supply_chain_data(data_dict.get('supply_chain', pd.DataFrame()))\n",
    "\n",
    "\n",
    "        # --- 4. Save Intermediate Data Analysis ---\n",
    "        print(\"\\n--- Saving Intermediate Analysis Data ---\")\n",
    "        try:\n",
    "            intermediate_data_saved = False\n",
    "            if trending_tags:\n",
    "                pd.DataFrame(trending_tags, columns=['tag', 'score']).to_csv('trending_tags.csv', index=False)\n",
    "                print(f\"Saved 'trending_tags.csv' ({len(trending_tags)} tags)\")\n",
    "                intermediate_data_saved = True\n",
    "            if product_gaps:\n",
    "                pd.DataFrame(product_gaps, columns=['product', 'mentions']).to_csv('product_gaps.csv', index=False)\n",
    "                print(f\"Saved 'product_gaps.csv' ({len(product_gaps)} gaps)\")\n",
    "                intermediate_data_saved = True\n",
    "            if trending_ingredients:\n",
    "                pd.DataFrame(trending_ingredients, columns=['ingredient', 'mentions']).to_csv('trending_ingredients.csv', index=False)\n",
    "                print(f\"Saved 'trending_ingredients.csv' ({len(trending_ingredients)} ingredients)\")\n",
    "                intermediate_data_saved = True\n",
    "            # Save new signal summaries\n",
    "            if top_amazon_products:\n",
    "                pd.DataFrame({'product_id': top_amazon_products}).to_csv('top_amazon_products.csv', index=False)\n",
    "                print(f\"Saved 'top_amazon_products.csv' ({len(top_amazon_products)} products)\")\n",
    "                intermediate_data_saved = True\n",
    "            if top_categories:\n",
    "                pd.DataFrame({'category': top_categories}).to_csv('top_categories.csv', index=False)\n",
    "                print(f\"Saved 'top_categories.csv' ({len(top_categories)} categories)\")\n",
    "                intermediate_data_saved = True\n",
    "            if top_brands:\n",
    "                pd.DataFrame({'brand': top_brands}).to_csv('top_brands.csv', index=False)\n",
    "                print(f\"Saved 'top_brands.csv' ({len(top_brands)} brands)\")\n",
    "                intermediate_data_saved = True\n",
    "            if successful_products:\n",
    "                pd.DataFrame({'product_name': successful_products}).to_csv('successful_products.csv', index=False)\n",
    "                print(f\"Saved 'successful_products.csv' ({len(successful_products)} products)\")\n",
    "                intermediate_data_saved = True\n",
    "            if top_supply_types:\n",
    "                pd.DataFrame({'product_type': top_supply_types}).to_csv('top_supply_types.csv', index=False)\n",
    "                print(f\"Saved 'top_supply_types.csv' ({len(top_supply_types)} types)\")\n",
    "                intermediate_data_saved = True\n",
    "\n",
    "            if not intermediate_data_saved:\n",
    "                print(\"No intermediate data to save.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save all intermediate CSV files: {e}\")\n",
    "\n",
    "\n",
    "        # --- 5. Generate Recommendations using DeepSeek via OpenRouter ---\n",
    "        print(\"\\n--- Generating Final Product Recommendations ---\")\n",
    "        recommendations = generate_deepseek_recommendations(\n",
    "            product_gaps, trending_ingredients, trending_tags,\n",
    "            top_amazon_products, top_categories, top_brands,\n",
    "            successful_products, top_supply_types, successful_brands_from_list\n",
    "        )\n",
    "\n",
    "        # --- 6. Generate Revenue Forecasts ---\n",
    "        print(\"\\n--- Generating Revenue Forecasts ---\")\n",
    "        if recommendations:\n",
    "            # Convert recommendations to DataFrame for forecasting\n",
    "            temp_rec_df = pd.DataFrame(recommendations)\n",
    "\n",
    "            # Apply ML forecasting\n",
    "            forecasted_df = forecast_product_revenue_and_margin(\n",
    "                temp_rec_df,\n",
    "                data_dict.get('top_products', pd.DataFrame()),\n",
    "                data_dict.get('supply_chain', pd.DataFrame())\n",
    "            )\n",
    "\n",
    "            # Add context columns to forecasted data\n",
    "            forecasted_df['analysis_context_trending_tags'] = str([tag for tag, score in trending_tags[:5]]) if trending_tags else \"[]\"\n",
    "            forecasted_df['analysis_context_top_product_gaps'] = str([product for product, count in product_gaps[:5]]) if product_gaps else \"[]\"\n",
    "            forecasted_df['analysis_context_top_ingredients'] = str([ingredient for ingredient, count in trending_ingredients[:5]]) if trending_ingredients else \"[]\"\n",
    "            forecasted_df['analysis_context_top_categories'] = str(top_categories[:5]) if top_categories else \"[]\"\n",
    "            forecasted_df['analysis_context_top_brands'] = str(top_brands[:5]) if top_brands else \"[]\"\n",
    "            forecasted_df['analysis_context_supply_types'] = str(top_supply_types[:5]) if top_supply_types else \"[]\"\n",
    "\n",
    "            # Save forecasted recommendations\n",
    "            output_filename = 'beauty_innovation_recommendations_with_forecasts.csv'\n",
    "            forecasted_df.to_csv(output_filename, index=False)\n",
    "            print(f\"\\n--- Analysis Complete! ---\")\n",
    "            print(f\"Final recommendations with forecasts saved to '{output_filename}' ({len(forecasted_df)} products)\")\n",
    "\n",
    "            # Display top forecasted products\n",
    "            print(\"\\n--- Top 5 Forecasted Products by Revenue Potential ---\")\n",
    "            top_forecasted = forecasted_df.nlargest(5, 'forecasted_yearly_revenue')\n",
    "            for i, (_, product) in enumerate(top_forecasted.iterrows()):\n",
    "                print(f\"{i+1}. {product['product_name']} - {product['category']}\")\n",
    "                print(\".0f\")\n",
    "                print(\".1f\")\n",
    "                print(f\"   Break-even: {product['break_even_months']:.1f} months\")\n",
    "                print(f\"   Investment: {product['investment_recommendation']}\")\n",
    "                print(f\"   Key Ingredients: {product.get('key_ingredients', 'N/A')}\")\n",
    "                print(f\"   Target Market: {product.get('target_market', 'N/A')}\\n\")\n",
    "        else:\n",
    "            print(\"\\n--- Analysis Complete ---\")\n",
    "            print(\"No final recommendations were generated.\")\n",
    "\n",
    "\n",
    "        # --- 7. Display Sample Results ---\n",
    "        if recommendations:\n",
    "            print(\"\\n--- Summary of All Forecasted Products ---\")\n",
    "            print(f\"Total Products: {len(forecasted_df)}\")\n",
    "            print(\".0f\")\n",
    "            print(\".0f\")\n",
    "            print(f\"Average Break-even: {forecasted_df['break_even_months'].mean():.1f} months\")\n",
    "            print(f\"Top Investment Recommendations: {forecasted_df['investment_recommendation'].value_counts().to_dict()}\")\n",
    "        else:\n",
    "            print(\"\\nNo recommendations available to display.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in the main function: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if OPENROUTER_API_KEY == \"<OPENROUTER_API_KEY>\" or not OPENROUTER_API_KEY:\n",
    "        print(\"ERROR: Please replace '<OPENROUTER_API_KEY>' with your actual OpenRouter API key in the script.\")\n",
    "    else:\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 33019,
     "sourceId": 43260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3169086,
     "sourceId": 5490896,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5666891,
     "sourceId": 9349131,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8128629,
     "sourceId": 12851871,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 258022563,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 258499969,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12393.540897,
   "end_time": "2025-08-28T08:56:09.289252",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-28T05:29:35.748355",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
