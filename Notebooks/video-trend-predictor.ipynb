{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a05feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T06:32:41.261079Z",
     "iopub.status.busy": "2025-08-29T06:32:41.260789Z",
     "iopub.status.idle": "2025-08-29T06:39:44.492549Z",
     "shell.execute_reply": "2025-08-29T06:39:44.491375Z"
    },
    "papermill": {
     "duration": 423.237015,
     "end_time": "2025-08-29T06:39:44.494190",
     "exception": false,
     "start_time": "2025-08-29T06:32:41.257175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (92759, 15) (39938, 13)\n",
      "Trainable rows: (39421, 13)\n",
      "\n",
      "=== Emerging (metadata-only) ===\n",
      "F1: 0.81\n",
      "\n",
      "=== Stage (metadata-only) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Decaying       0.99      0.95      0.97      7730\n",
      "     Peaking       0.12      0.37      0.18        68\n",
      "      Rising       0.09      0.32      0.14        87\n",
      "\n",
      "    accuracy                           0.93      7885\n",
      "   macro avg       0.40      0.54      0.43      7885\n",
      "weighted avg       0.97      0.93      0.95      7885\n",
      "\n",
      "\n",
      "=== Volume (metadata-only) ===\n",
      "RMSE: 0.114\n",
      "\n",
      "Saved metadata-only models to: /kaggle/working\n"
     ]
    }
   ],
   "source": [
    "# === Cell M1: Train metadata-only models using labels from all_signals_combined.csv ===\n",
    "import pandas as pd, numpy as np, re, ast, joblib, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, f1_score, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Paths (adjust if your videos.csv lives elsewhere) ----\n",
    "VIDEOS_PATH = Path(\"/kaggle/input/datathon-loreal/videos.csv\")\n",
    "ALL_SIG_PATH = Path(\"/kaggle/input/fusion-engine/all_signals_combined.csv\")\n",
    "assert VIDEOS_PATH.exists(), f\"Missing {VIDEOS_PATH}\"\n",
    "assert ALL_SIG_PATH.exists(), f\"Missing {ALL_SIG_PATH}\"\n",
    "\n",
    "# ---- Load ----\n",
    "vid = pd.read_csv(VIDEOS_PATH)\n",
    "sig = pd.read_csv(ALL_SIG_PATH)\n",
    "print(\"Loaded:\", vid.shape, sig.shape)\n",
    "\n",
    "# ---- Minimal columns & parsing helpers ----\n",
    "def parse_iso_duration_to_seconds(s):\n",
    "    if not isinstance(s, str): return np.nan\n",
    "    m = re.match(r\"^PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?$\", s)\n",
    "    if not m: \n",
    "        # allow plain seconds (e.g., \"30\")\n",
    "        try: return float(s)\n",
    "        except: return np.nan\n",
    "    h = int(m.group(1)) if m.group(1) else 0\n",
    "    m_ = int(m.group(2)) if m.group(2) else 0\n",
    "    s_ = int(m.group(3)) if m.group(3) else 0\n",
    "    return h*3600 + m_*60 + s_\n",
    "\n",
    "def tags_to_text(x):\n",
    "    # videos.csv may store tags as JSON-like list strings\n",
    "    try:\n",
    "        if isinstance(x, str) and x.startswith(\"[\") and x.endswith(\"]\"):\n",
    "            arr = ast.literal_eval(x)\n",
    "            if isinstance(arr, list): return \" \".join(map(str, arr))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(x) if isinstance(x, str) else \"\"\n",
    "\n",
    "# Ensure required columns exist\n",
    "for c in [\"videoId\",\"title\",\"description\",\"tags\",\"contentDuration\",\"viewCount\",\"likeCount\",\"commentCount\"]:\n",
    "    if c not in vid.columns: vid[c] = np.nan\n",
    "\n",
    "# Build text + numeric metadata\n",
    "vid[\"duration_seconds\"] = vid[\"contentDuration\"].apply(parse_iso_duration_to_seconds)\n",
    "vid[\"tags_text\"] = vid[\"tags\"].apply(tags_to_text)\n",
    "for c in [\"title\",\"description\",\"tags_text\"]:\n",
    "    vid[c] = vid[c].fillna(\"\")\n",
    "vid[\"text_all\"]  = (vid[\"title\"] + \" \" + vid[\"description\"] + \" \" + vid[\"tags_text\"]).str.strip()\n",
    "vid[\"title_len\"] = vid[\"title\"].str.len()\n",
    "vid[\"desc_len\"]  = vid[\"description\"].str.len()\n",
    "vid[\"tags_len\"]  = vid[\"tags_text\"].str.len()\n",
    "\n",
    "meta_cols = [\"videoId\",\"text_all\",\"duration_seconds\",\"viewCount\",\"likeCount\",\"commentCount\",\"title_len\",\"desc_len\",\"tags_len\"]\n",
    "meta = vid[meta_cols].copy()\n",
    "\n",
    "# ---- Derive labels from all_signals (NO leakage back into features) ----\n",
    "# Prefer composite_score for Emerging; fallback to predicted_future_score if needed\n",
    "source_for_emerging = \"composite_score\" if \"composite_score\" in sig.columns else \"predicted_future_score\"\n",
    "assert source_for_emerging in sig.columns, \"Need composite_score or predicted_future_score in all_signals.\"\n",
    "\n",
    "# Stage heuristic uses momentum vs decay signals when available\n",
    "mom_cols = [c for c in sig.columns if re.search(r'hawkes|tbi_', c, re.IGNORECASE)]\n",
    "dec_cols = [c for c in sig.columns if re.search(r'decay', c, re.IGNORECASE)]\n",
    "\n",
    "def stage_from_signals_row(row):\n",
    "    mom_vals = [row[c] for c in mom_cols if c in sig.columns]\n",
    "    dec_vals = [row[c] for c in dec_cols if c in sig.columns]\n",
    "    mom = np.nanmean(mom_vals) if len(mom_vals) else np.nan\n",
    "    dec = np.nanmean(dec_vals) if len(dec_vals) else np.nan\n",
    "    if np.isnan(mom) or np.isnan(dec): return \"Peaking\"\n",
    "    if mom > dec * 1.10: return \"Rising\"\n",
    "    if dec > mom * 1.10: return \"Decaying\"\n",
    "    return \"Peaking\"\n",
    "\n",
    "lab = sig[[\"videoId\", source_for_emerging]].copy()\n",
    "lab[\"label_emerging\"] = (lab[source_for_emerging] >= np.nanpercentile(lab[source_for_emerging], 80)).astype(int)\n",
    "\n",
    "if len(mom_cols) or len(dec_cols):\n",
    "    tmp = sig[[\"videoId\"] + list(set(mom_cols + dec_cols))].copy()\n",
    "    tmp[\"label_stage\"] = tmp.apply(stage_from_signals_row, axis=1)\n",
    "    lab = lab.merge(tmp[[\"videoId\",\"label_stage\"]], on=\"videoId\", how=\"left\")\n",
    "else:\n",
    "    lab[\"label_stage\"] = \"Peaking\"\n",
    "\n",
    "# Volume target = predicted_future_score if present, else composite_score\n",
    "vol_target = \"predicted_future_score\" if \"predicted_future_score\" in sig.columns else \"composite_score\"\n",
    "lab[\"target_volume_reg\"] = sig.set_index(\"videoId\")[vol_target].reindex(lab[\"videoId\"]).values\n",
    "\n",
    "# ---- Join metadata with labels ----\n",
    "df = meta.merge(lab, on=\"videoId\", how=\"inner\").dropna(subset=[\"text_all\",\"duration_seconds\"])\n",
    "print(\"Trainable rows:\", df.shape)\n",
    "\n",
    "# ---- Train/test (time-agnostic split for simplicity) ----\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.20, random_state=42, stratify=df[\"label_emerging\"])\n",
    "\n",
    "TEXT_COL = \"text_all\"\n",
    "NUM_COLS = [\"duration_seconds\",\"viewCount\",\"likeCount\",\"commentCount\",\"title_len\",\"desc_len\",\"tags_len\"]\n",
    "\n",
    "# Preprocessor: TF-IDF on text + scaled numeric\n",
    "ct = ColumnTransformer([\n",
    "    (\"text\", TfidfVectorizer(max_features=12000, ngram_range=(1,2)), TEXT_COL),\n",
    "    (\"num\",  Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                       (\"scl\", StandardScaler(with_mean=False))]),\n",
    "     NUM_COLS)\n",
    "])\n",
    "\n",
    "# Models: LogisticRegression(saga) for Emerging/Stage, SGDRegressor on log1p(target) for Volume\n",
    "clf_em = Pipeline([\n",
    "    (\"prep\", ct),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"saga\", penalty=\"l2\",\n",
    "        max_iter=5000, class_weight=\"balanced\", n_jobs=-1, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf_st = Pipeline([\n",
    "    (\"prep\", ct),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"saga\", penalty=\"l2\", multi_class=\"multinomial\",\n",
    "        max_iter=5000, class_weight=\"balanced\", n_jobs=-1, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "reg_vo = Pipeline([\n",
    "    (\"prep\", ct),\n",
    "    (\"reg\", TransformedTargetRegressor(\n",
    "        regressor=SGDRegressor(loss=\"squared_error\", penalty=\"l2\",\n",
    "                               alpha=1e-4, max_iter=3000, tol=1e-4, random_state=42),\n",
    "        func=np.log1p, inverse_func=np.expm1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "clf_em.fit(train_df[[TEXT_COL]+NUM_COLS], train_df[\"label_emerging\"].astype(int))\n",
    "clf_st.fit(train_df[[TEXT_COL]+NUM_COLS], train_df[\"label_stage\"].astype(str))\n",
    "reg_vo.fit(train_df[[TEXT_COL]+NUM_COLS], train_df[\"target_volume_reg\"].astype(float))\n",
    "\n",
    "# Quick eval\n",
    "from sklearn.metrics import accuracy_score\n",
    "pred_em = clf_em.predict(test_df[[TEXT_COL]+NUM_COLS])\n",
    "print(\"\\n=== Emerging (metadata-only) ===\")\n",
    "print(\"F1:\", round(f1_score(test_df[\"label_emerging\"], pred_em), 3))\n",
    "\n",
    "pred_st = clf_st.predict(test_df[[TEXT_COL]+NUM_COLS])\n",
    "print(\"\\n=== Stage (metadata-only) ===\")\n",
    "print(classification_report(test_df[\"label_stage\"], pred_st, zero_division=0))\n",
    "\n",
    "pred_vo = reg_vo.predict(test_df[[TEXT_COL]+NUM_COLS])\n",
    "rmse = float(np.sqrt(mean_squared_error(test_df[\"target_volume_reg\"], pred_vo)))\n",
    "print(\"\\n=== Volume (metadata-only) ===\")\n",
    "print(\"RMSE:\", round(rmse, 3))\n",
    "\n",
    "# Save PKLs (metadata-only)\n",
    "MODEL_DIR = Path(\"/kaggle/working\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(clf_em, str(MODEL_DIR/\"meta_emerging.pkl\"))\n",
    "joblib.dump(clf_st, str(MODEL_DIR/\"meta_stage.pkl\"))\n",
    "joblib.dump(reg_vo, str(MODEL_DIR/\"meta_volume.pkl\"))\n",
    "print(\"\\nSaved metadata-only models to:\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c2192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T03:22:11.839122Z",
     "iopub.status.busy": "2025-08-29T03:22:11.838265Z",
     "iopub.status.idle": "2025-08-29T03:22:39.284338Z",
     "shell.execute_reply": "2025-08-29T03:22:39.283144Z",
     "shell.execute_reply.started": "2025-08-29T03:22:11.839075Z"
    },
    "papermill": {
     "duration": 0.001629,
     "end_time": "2025-08-29T06:39:44.498286",
     "exception": false,
     "start_time": "2025-08-29T06:39:44.496657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8128629,
     "sourceId": 12851871,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 258022563,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 258228149,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 258499969,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 429.881891,
   "end_time": "2025-08-29T06:39:45.421988",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-29T06:32:35.540097",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
