{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b4f6f5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-27T13:00:12.400807Z",
     "iopub.status.busy": "2025-08-27T13:00:12.400484Z",
     "iopub.status.idle": "2025-08-27T13:00:48.029309Z",
     "shell.execute_reply": "2025-08-27T13:00:48.028277Z"
    },
    "papermill": {
     "duration": 35.637388,
     "end_time": "2025-08-27T13:00:48.032522",
     "exception": false,
     "start_time": "2025-08-27T13:00:12.395134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading comments_enriched.parquet...\n",
      "[INFO] Loaded dataset shape: (4725012, 17)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "INPUT_PATH = \"/kaggle/input/data-cleaning/comments_enriched.parquet\"\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "SEGMENT_LABELS_PATH = f\"{OUTPUT_DIR}/segments_labels.csv\"\n",
    "ARTIFACTS_DIR = f\"{OUTPUT_DIR}/artifacts\"\n",
    "Path(ARTIFACTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===== 1. LOAD DATA =====\n",
    "print(f\"[INFO] Loading comments_enriched.parquet...\")\n",
    "df = pd.read_parquet(INPUT_PATH)\n",
    "print(f\"[INFO] Loaded dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1ab983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T13:00:48.040764Z",
     "iopub.status.busy": "2025-08-27T13:00:48.040266Z",
     "iopub.status.idle": "2025-08-27T13:03:45.665395Z",
     "shell.execute_reply": "2025-08-27T13:03:45.664589Z"
    },
    "papermill": {
     "duration": 177.631242,
     "end_time": "2025-08-27T13:03:45.667139",
     "exception": false,
     "start_time": "2025-08-27T13:00:48.035897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting feature engineering...\n"
     ]
    }
   ],
   "source": [
    "# ===== 2. FEATURE ENGINEERING =====\n",
    "print(\"[INFO] Starting feature engineering...\")\n",
    "\n",
    "# Text length feature\n",
    "df['text_len'] = df['text_norm'].str.len().fillna(0)\n",
    "\n",
    "# Time-of-day buckets\n",
    "def time_of_day(hour):\n",
    "    if 5 <= hour < 12: return 'morning'\n",
    "    elif 12 <= hour < 17: return 'afternoon'\n",
    "    elif 17 <= hour < 22: return 'evening'\n",
    "    else: return 'night'\n",
    "df['time_of_day'] = df['hour'].apply(time_of_day)\n",
    "\n",
    "# Slang detection\n",
    "def detect_slang(text):\n",
    "    if not isinstance(text, str) or len(text) < 5:\n",
    "        return 0\n",
    "    slang_indicators = [\n",
    "        r'\\bu\\b', r'\\br\\b', r'\\bl8r\\b', r'\\n2\\b', r'\\bthx\\b', r'\\bomg\\b', r'\\byo\\b',\n",
    "        r'\\bbtw\\b', r'\\bimo\\b', r'\\bfomo\\b', r'\\bsmh\\b', r'\\btbh\\b', r'\\bily\\b',\n",
    "        r'\\bfr\\b', r'\\bngl\\b', r'\\bdeez\\b', r'\\bwyd\\b', r'\\bwyll\\b', r'\\bwyg\\b',\n",
    "        r'\\bperiodt\\b', r'\\bstan\\b', r'\\bbussin\\b', r'\\bsus\\b', r'\\bglowup\\b'\n",
    "    ]\n",
    "    score = sum(1 for p in slang_indicators if re.search(p, text.lower()))\n",
    "    if len(text) < 30:\n",
    "        score += 1\n",
    "    return min(score, 5)\n",
    "df['slang_score'] = df['text_norm'].apply(detect_slang)\n",
    "\n",
    "# Topic extraction from hashtags\n",
    "def extract_topics(hashtags):\n",
    "    if hashtags is None or (isinstance(hashtags, float) and pd.isna(hashtags)):\n",
    "        return []\n",
    "    if hasattr(hashtags, 'tolist'):\n",
    "        hashtags = hashtags.tolist()\n",
    "    if isinstance(hashtags, str):\n",
    "        if hashtags.startswith('[') and hashtags.endswith(']'):\n",
    "            try:\n",
    "                import ast\n",
    "                hashtags = ast.literal_eval(hashtags)\n",
    "            except:\n",
    "                hashtags = [hashtags]\n",
    "        else:\n",
    "            hashtags = [hashtags]\n",
    "    if not isinstance(hashtags, list):\n",
    "        return []\n",
    "    common_tags = {'love', 'like', 'comment', 'share', 'video', 'youtube', 'subscribe'}\n",
    "    beauty_topics = {\n",
    "        'skincare': ['skincare', 'skincareroutine', 'skincaretips', 'skincarehacks'],\n",
    "        'makeup': ['makeup', 'makeuproutine', 'makeuptutorial', 'makeuplook'],\n",
    "        'haircare': ['haircare', 'hairroutin', 'hairtutorial', 'hairgoals'],\n",
    "        'beauty': ['beauty', 'beautytips', 'beautyhacks', 'beautyblogger'],\n",
    "        'fashion': ['fashion', 'outfit', 'style', 'outfitideas'],\n",
    "        'lifestyle': ['lifestyle', 'dailyroutine', 'dayinmylife', 'lifetips']\n",
    "    }\n",
    "    topics = []\n",
    "    for tag in hashtags:\n",
    "        if not isinstance(tag, str):\n",
    "            continue\n",
    "        tag = tag.lower().replace('#', '')\n",
    "        if tag in common_tags:\n",
    "            continue\n",
    "        for category, keywords in beauty_topics.items():\n",
    "            if any(keyword in tag for keyword in keywords):\n",
    "                topics.append(category)\n",
    "                break\n",
    "        else:\n",
    "            topics.append(tag)\n",
    "    return list(set(topics))\n",
    "df['topics'] = df['hashtags'].apply(extract_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db914a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T13:03:45.675827Z",
     "iopub.status.busy": "2025-08-27T13:03:45.675467Z",
     "iopub.status.idle": "2025-08-27T13:04:53.146772Z",
     "shell.execute_reply": "2025-08-27T13:04:53.145419Z"
    },
    "papermill": {
     "duration": 67.480268,
     "end_time": "2025-08-27T13:04:53.151159",
     "exception": false,
     "start_time": "2025-08-27T13:03:45.670891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preparing features for GMM...\n",
      "[OK] Feature matrix shape: (4725012, 76)\n"
     ]
    }
   ],
   "source": [
    "# ===== 3. FEATURE SELECTION & PREPROCESSING =====\n",
    "print(\"[INFO] Preparing features for GMM...\")\n",
    "\n",
    "# Base features\n",
    "features = df[['emoji_count', 'text_len', 'hour', 'lang', 'time_of_day']].copy()\n",
    "\n",
    "# --- ENHANCED FEATURE ENGINEERING ---\n",
    "# Emoji density (better than raw count)\n",
    "word_counts = df['text_norm'].str.split().str.len().replace(0, 1)\n",
    "features['emoji_per_word'] = (features['emoji_count'] / word_counts).fillna(0)\n",
    "features['is_high_emoji'] = (features['emoji_count'] >= 2).astype(int)\n",
    "\n",
    "# Short comment indicator\n",
    "features['is_short_comment'] = (df['text_len'] <= 30).astype(int)\n",
    "\n",
    "# Slang count\n",
    "slang_terms = [\n",
    "    'bussin', 'rizz', 'cap', 'no cap', 'gyat', 'gyatt', 'sigma', 'skibidi', 'grimace', 'fanum',\n",
    "    'periodt', 'slay', 'delulu', 'cheugy', 'vibe', 'ate', 'snatched', 'glowup', 'stan', 'sus',\n",
    "    'fr', 'tbh', 'omg', 'u', 'r', 'l8r', 'wyd', 'wyll', 'ily', 'bff', 'fomo', 'smh'\n",
    "]\n",
    "df['slang_count'] = df['text_norm'].str.lower().apply(\n",
    "    lambda text: sum(1 for term in slang_terms if term in text)\n",
    ")\n",
    "features['slang_count'] = df['slang_count']\n",
    "features['is_high_slang'] = (df['slang_count'] >= 1).astype(int)\n",
    "\n",
    "# Night owl behavior\n",
    "features['is_night_owl'] = ((df['hour'] >= 22) | (df['hour'] <= 4)).astype(int)\n",
    "\n",
    "# Circular time encoding (important for GMM)\n",
    "features['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "features['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Beauty keyword focus\n",
    "beauty_keywords = [\n",
    "    'skincare', 'makeup', 'routine', 'tutorial', 'review', 'hack', 'tip', 'look', 'glow', 'glass',\n",
    "    'filter', 'foundation', 'concealer', 'eyeshadow', 'lipstick', 'blush', 'serum', 'moisturizer',\n",
    "    'cleanser', 'toner', 'mask', 'acne', 'pimple', 'dark spot', 'brighten', 'exfoliate'\n",
    "]\n",
    "df['has_beauty_keyword'] = df['text_norm'].str.lower().apply(\n",
    "    lambda text: any(kw in text for kw in beauty_keywords)\n",
    ").astype(int)\n",
    "features['has_beauty_keyword'] = df['has_beauty_keyword']\n",
    "\n",
    "# Long-form content\n",
    "features['is_long_comment'] = (df['text_len'] > 100).astype(int)\n",
    "\n",
    "# Drop raw features\n",
    "features.drop(['emoji_count'], axis=1, inplace=True)\n",
    "\n",
    "# Final feature sets\n",
    "numeric_features = ['emoji_per_word', 'slang_count', 'hour_sin', 'hour_cos']\n",
    "categorical_features = [\n",
    "    'is_high_emoji', 'is_short_comment', 'is_high_slang',\n",
    "    'is_night_owl', 'is_long_comment', 'has_beauty_keyword',\n",
    "    'lang', 'time_of_day'\n",
    "]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# Fit and transform\n",
    "X = preprocessor.fit_transform(features)\n",
    "print(f\"[OK] Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b746108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T13:04:53.158598Z",
     "iopub.status.busy": "2025-08-27T13:04:53.158283Z",
     "iopub.status.idle": "2025-08-27T13:05:24.550197Z",
     "shell.execute_reply": "2025-08-27T13:05:24.549355Z"
    },
    "papermill": {
     "duration": 31.400565,
     "end_time": "2025-08-27T13:05:24.554887",
     "exception": false,
     "start_time": "2025-08-27T13:04:53.154322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finding optimal number of components with BIC...\n",
      "[INFO] Sampling 100000 rows...\n",
      "[INFO] Fitting GMM with n_components=3...\n",
      "  → BIC = -49974632.39\n",
      "[INFO] Fitting GMM with n_components=4...\n",
      "  → BIC = -52188005.14\n",
      "[INFO] Fitting GMM with n_components=5...\n",
      "  → BIC = -51069638.67\n",
      "[OK] Selected optimal k = 4 (lowest BIC)\n"
     ]
    }
   ],
   "source": [
    "# ===== 4. OPTIMAL COMPONENT SELECTION (on sample) =====\n",
    "print(\"[INFO] Finding optimal number of components with BIC...\")\n",
    "\n",
    "# Downsample for efficiency\n",
    "SAMPLE_SIZE = 100_000\n",
    "if len(X) > SAMPLE_SIZE:\n",
    "    print(f\"[INFO] Sampling {SAMPLE_SIZE} rows...\")\n",
    "    np.random.seed(42)\n",
    "    sample_idx = np.random.choice(len(X), SAMPLE_SIZE, replace=False)\n",
    "    X_sample = X[sample_idx]\n",
    "else:\n",
    "    X_sample = X\n",
    "\n",
    "# Test components using BIC (better than silhouette for GMM)\n",
    "n_components_range = range(3, 6)\n",
    "bics = []\n",
    "\n",
    "for n in n_components_range:\n",
    "    print(f\"[INFO] Fitting GMM with n_components={n}...\")\n",
    "    gmm = GaussianMixture(n_components=n, covariance_type='full', random_state=42, max_iter=500)\n",
    "    gmm.fit(X_sample)\n",
    "    bics.append(gmm.bic(X_sample))\n",
    "    print(f\"  → BIC = {bics[-1]:.2f}\")\n",
    "\n",
    "optimal_k = n_components_range[np.argmin(bics)]\n",
    "print(f\"[OK] Selected optimal k = {optimal_k} (lowest BIC)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4e4d29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T13:05:24.563177Z",
     "iopub.status.busy": "2025-08-27T13:05:24.562857Z",
     "iopub.status.idle": "2025-08-27T13:10:45.326228Z",
     "shell.execute_reply": "2025-08-27T13:10:45.325328Z"
    },
    "papermill": {
     "duration": 320.769625,
     "end_time": "2025-08-27T13:10:45.328119",
     "exception": false,
     "start_time": "2025-08-27T13:05:24.558494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fitting final GMM on full data with n_components=4...\n"
     ]
    }
   ],
   "source": [
    "# ===== 5. FINAL GMM FITTING ON FULL DATA =====\n",
    "print(f\"[INFO] Fitting final GMM on full data with n_components={optimal_k}...\")\n",
    "gmm_final = GaussianMixture(\n",
    "    n_components=optimal_k,\n",
    "    covariance_type='full',\n",
    "    random_state=42,\n",
    "    max_iter=500\n",
    ")\n",
    "gmm_final.fit(X)\n",
    "\n",
    "# Soft assignment (probabilities) + hard labels\n",
    "proba = gmm_final.predict_proba(X)  # shape: (N, k)\n",
    "labels = gmm_final.predict(X)\n",
    "confidence = proba.max(axis=1)  # Highest probability = confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d98bd986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T13:10:45.338493Z",
     "iopub.status.busy": "2025-08-27T13:10:45.338101Z",
     "iopub.status.idle": "2025-08-27T13:10:45.356151Z",
     "shell.execute_reply": "2025-08-27T13:10:45.355165Z"
    },
    "papermill": {
     "duration": 0.024873,
     "end_time": "2025-08-27T13:10:45.357660",
     "exception": false,
     "start_time": "2025-08-27T13:10:45.332787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Interpreting clusters using GMM means (Robust Heuristics)...\n",
      "[INFO] Cluster centers shape: (4, 76)\n",
      "\n",
      "[INFO] Analyzing raw cluster characteristics...\n",
      "  Cluster 0: slang=-1.21, emoji/word=0.31, hour=5.3, is_long_text=1.00, has_beauty_kw=0.97\n",
      "  Cluster 1: slang=0.49, emoji/word=-0.14, hour=10.6, is_long_text=0.88, has_beauty_kw=0.80\n",
      "  Cluster 2: slang=0.56, emoji/word=-0.18, hour=22.6, is_long_text=0.86, has_beauty_kw=0.81\n",
      "  Cluster 3: slang=-1.20, emoji/word=0.43, hour=16.1, is_long_text=0.98, has_beauty_kw=0.97\n",
      "\n",
      "[INFO] Cluster-to-Segment Mapping (Robust): {0: 'InterestDriven', 1: 'Millennial', 2: 'GenZ', 3: 'InterestDriven'}\n",
      "[OK] Assigned Gen Z to Cluster 2\n",
      "[OK] Assigned Millennial to Cluster 1\n"
     ]
    }
   ],
   "source": [
    "# ===== 6. INTERPRET CLUSTERS & ASSIGN SEGMENTS (Robust Version) =====\n",
    "print(\"[INFO] Interpreting clusters using GMM means (Robust Heuristics)...\")\n",
    "\n",
    "# Get cluster centers\n",
    "cluster_centers = gmm_final.means_\n",
    "print(f\"[INFO] Cluster centers shape: {cluster_centers.shape}\")\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "numeric_feature_names = numeric_features\n",
    "categorical_feature_names_out = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "feature_names = list(numeric_feature_names) + list(categorical_feature_names_out)\n",
    "\n",
    "# Create index map\n",
    "feature_index_map = {name: i for i, name in enumerate(feature_names)}\n",
    "\n",
    "# --- Extract key metrics for all clusters first ---\n",
    "cluster_profiles = []\n",
    "print(\"\\n[INFO] Analyzing raw cluster characteristics...\")\n",
    "for i in range(optimal_k):\n",
    "    center = cluster_centers[i]\n",
    "\n",
    "    # Extract features\n",
    "    slang_count = center[feature_index_map['slang_count']]\n",
    "    emoji_per_word = center[feature_index_map['emoji_per_word']]\n",
    "    hour_sin = center[feature_index_map['hour_sin']]\n",
    "    hour_cos = center[feature_index_map['hour_cos']]\n",
    "    \n",
    "    # Convert circular time to hour (0–23)\n",
    "    hour_angle_rad = np.arctan2(hour_sin, hour_cos)\n",
    "    hour = (hour_angle_rad * 12 / np.pi) % 24\n",
    "    if hour < 0:\n",
    "        hour += 24\n",
    "\n",
    "    # Find binary feature indices (robust lookup)\n",
    "    def find_feature(name_part):\n",
    "        for name in feature_names:\n",
    "            if name_part in name:\n",
    "                return name\n",
    "        return None\n",
    "\n",
    "    long_comment_feature = find_feature('is_long_comment')\n",
    "    beauty_keyword_feature = find_feature('has_beauty_keyword')\n",
    "    \n",
    "    is_long_comment_prob = center[feature_index_map[long_comment_feature]] if long_comment_feature else 0.0\n",
    "    has_beauty_keyword_prob = center[feature_index_map[beauty_keyword_feature]] if beauty_keyword_feature else 0.0\n",
    "\n",
    "    # Store profile\n",
    "    profile = {\n",
    "        'cluster_id': i,\n",
    "        'slang_count': slang_count,\n",
    "        'emoji_per_word': emoji_per_word,\n",
    "        'hour': hour,\n",
    "        'is_long_comment': is_long_comment_prob,\n",
    "        'has_beauty_keyword': has_beauty_keyword_prob\n",
    "    }\n",
    "    cluster_profiles.append(profile)\n",
    "    \n",
    "    # Print diagnostics\n",
    "    print(f\"  Cluster {i}: \"\n",
    "          f\"slang={slang_count:.2f}, \"\n",
    "          f\"emoji/word={emoji_per_word:.2f}, \"\n",
    "          f\"hour={hour:.1f}, \"\n",
    "          f\"is_long_text={is_long_comment_prob:.2f}, \"\n",
    "          f\"has_beauty_kw={has_beauty_keyword_prob:.2f}\")\n",
    "\n",
    "# --- Assign labels based on relative differences ---\n",
    "# 1. Find Gen Z candidate: highest slang, late night, lowest emoji\n",
    "genz_score = []\n",
    "for p in cluster_profiles:\n",
    "    # Gen Z score: high slang, late hour (penalize early hours), low emoji\n",
    "    score = p['slang_count'] + (1 if (p['hour'] > 20 or p['hour'] < 5) else -1) - p['emoji_per_word']\n",
    "    genz_score.append((p['cluster_id'], score))\n",
    "\n",
    "# Sort by score descending, top candidate is Gen Z\n",
    "genz_score.sort(key=lambda x: x[1], reverse=True)\n",
    "genz_cluster_id = genz_score[0][0]\n",
    "\n",
    "# 2. Find Millennial candidates: daytime, moderate slang, moderate emojis\n",
    "millennial_score = []\n",
    "for p in cluster_profiles:\n",
    "    # Millennial score: daytime, moderate slang, moderate emojis\n",
    "    hour_score = 1 if (8 <= p['hour'] <= 18) else 0\n",
    "    slang_score = 1 if (0.1 < p['slang_count'] < 1.5) else 0\n",
    "    emoji_score = 1 if (p['emoji_per_word'] > -0.5) else 0 # More positive than others\n",
    "    score = hour_score + slang_score + emoji_score\n",
    "    millennial_score.append((p['cluster_id'], score))\n",
    "\n",
    "# Sort and pick the best non-Gen Z cluster\n",
    "millennial_score.sort(key=lambda x: x[1], reverse=True)\n",
    "millennial_cluster_id = None\n",
    "for cid, _ in millennial_score:\n",
    "    if cid != genz_cluster_id:\n",
    "        millennial_cluster_id = cid\n",
    "        break\n",
    "\n",
    "# 3. Assign labels\n",
    "segment_mapping = {}\n",
    "for p in cluster_profiles:\n",
    "    cid = p['cluster_id']\n",
    "    if cid == genz_cluster_id:\n",
    "        segment_mapping[cid] = \"GenZ\"\n",
    "    elif cid == millennial_cluster_id:\n",
    "        segment_mapping[cid] = \"Millennial\"\n",
    "    elif p['has_beauty_keyword'] > 0.7: # High topic focus\n",
    "        segment_mapping[cid] = \"InterestDriven\"\n",
    "    else:\n",
    "        segment_mapping[cid] = \"Other\"\n",
    "\n",
    "print(f\"\\n[INFO] Cluster-to-Segment Mapping (Robust): {segment_mapping}\")\n",
    "print(f\"[OK] Assigned Gen Z to Cluster {genz_cluster_id}\")\n",
    "if millennial_cluster_id is not None:\n",
    "    print(f\"[OK] Assigned Millennial to Cluster {millennial_cluster_id}\")\n",
    "else:\n",
    "    print(\"[WARNING] Could not confidently assign a Millennial cluster.\")\n",
    "\n",
    "# --- Final Validation ---\n",
    "genz_candidates = [cid for cid, seg in segment_mapping.items() if seg == \"GenZ\"]\n",
    "if not genz_candidates:\n",
    "    print(\"[CRITICAL] No Gen Z cluster assigned. Review data or features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e13bc83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T13:10:45.366194Z",
     "iopub.status.busy": "2025-08-27T13:10:45.365871Z",
     "iopub.status.idle": "2025-08-27T13:10:55.942867Z",
     "shell.execute_reply": "2025-08-27T13:10:55.941710Z"
    },
    "papermill": {
     "duration": 10.583123,
     "end_time": "2025-08-27T13:10:55.944369",
     "exception": false,
     "start_time": "2025-08-27T13:10:45.361246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating segments_labels.csv...\n",
      "[OK] Saved segments_labels.csv to /kaggle/working/segments_labels.csv\n",
      "      Shape: (4725012, 5)\n",
      "      Columns: ['commentId', 'videoId', 'segment_id', 'segment', 'confidence']\n",
      "      Sample rows:\n",
      "   commentId  videoId  segment_id         segment  confidence\n",
      "0    1781382    74288           2            GenZ         1.0\n",
      "1     289571    79618           1      Millennial         1.0\n",
      "2     569077    51826           1      Millennial         1.0\n",
      "3    2957962    58298           3  InterestDriven         1.0\n",
      "4     673093     1265           0  InterestDriven         1.0\n"
     ]
    }
   ],
   "source": [
    "# ===== 7. GENERATE segments_labels.csv =====\n",
    "print(\"[INFO] Generating segments_labels.csv...\")\n",
    "\n",
    "# Create per-commenter labels DataFrame\n",
    "# Select only the necessary base columns from the original df to avoid column conflicts\n",
    "# Assuming 'commentId' and 'videoId' are in the original df\n",
    "segment_labels = df[[\"commentId\", \"videoId\"]].copy()\n",
    "segment_labels[\"segment_id\"] = labels\n",
    "segment_labels[\"segment\"] = pd.Series(labels).map(segment_mapping)\n",
    "\n",
    "# Add confidence score (max probability from GMM)\n",
    "segment_labels[\"confidence\"] = confidence\n",
    "\n",
    "# Optional: Add back some key features for debugging/diagnostics if needed\n",
    "# segment_labels[\"slang_count\"] = df[\"slang_count\"]\n",
    "# segment_labels[\"emoji_per_word\"] = (df['emoji_count'] / df['text_norm'].str.split().str.len().replace(0, 1)).fillna(0)\n",
    "\n",
    "# Ensure output directory exists\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the per-commenter segment labels\n",
    "segment_labels.to_csv(SEGMENT_LABELS_PATH, index=False)\n",
    "print(f\"[OK] Saved segments_labels.csv to {SEGMENT_LABELS_PATH}\")\n",
    "print(f\"      Shape: {segment_labels.shape}\")\n",
    "print(f\"      Columns: {list(segment_labels.columns)}\")\n",
    "print(f\"      Sample rows:\")\n",
    "print(segment_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25178a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T13:10:55.953305Z",
     "iopub.status.busy": "2025-08-27T13:10:55.952951Z",
     "iopub.status.idle": "2025-08-27T13:11:21.813537Z",
     "shell.execute_reply": "2025-08-27T13:11:21.812430Z"
    },
    "papermill": {
     "duration": 25.866662,
     "end_time": "2025-08-27T13:11:21.815047",
     "exception": false,
     "start_time": "2025-08-27T13:10:55.948385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating segments_video.csv (aggregated per video)...\n",
      "[OK] Saved segments_video.csv to /kaggle/working/segments_video.csv\n",
      "      Shape: (39938, 7)\n",
      "      Columns: ['videoId', 'total_comments', 'genz_pct', 'millennial_pct', 'interest_pct', 'other_pct', 'top_interest']\n",
      "      Sample rows:\n",
      "   videoId  total_comments   genz_pct  millennial_pct  interest_pct  \\\n",
      "0        0             526   32.69962       33.460076     33.840304   \n",
      "1        1               1    0.00000        0.000000    100.000000   \n",
      "2        2               1  100.00000        0.000000      0.000000   \n",
      "3        6               5   80.00000       20.000000      0.000000   \n",
      "4        7               1    0.00000        0.000000    100.000000   \n",
      "\n",
      "   other_pct    top_interest  \n",
      "0        0.0  InterestDriven  \n",
      "1        0.0  InterestDriven  \n",
      "2        0.0            GenZ  \n",
      "3        0.0            GenZ  \n",
      "4        0.0  InterestDriven  \n"
     ]
    }
   ],
   "source": [
    "# ===== 8. GENERATE segments_video.csv (Aggregated per-video) =====\n",
    "print(\"[INFO] Generating segments_video.csv (aggregated per video)...\")\n",
    "\n",
    "# Aggregate to get the percentage mix per video\n",
    "# Using groupby and apply with lambda for flexible calculations\n",
    "video_segments = segment_labels.groupby(\"videoId\").agg(\n",
    "    total_comments=(\"commentId\", \"count\"),\n",
    "    genz_pct=(\"segment\", lambda x: (x == \"GenZ\").sum() / len(x) * 100),\n",
    "    millennial_pct=(\"segment\", lambda x: (x == \"Millennial\").sum() / len(x) * 100),\n",
    "    interest_pct=(\"segment\", lambda x: (x == \"InterestDriven\").sum() / len(x) * 100),\n",
    "    other_pct=(\"segment\", lambda x: (x == \"Other\").sum() / len(x) * 100),\n",
    "    top_interest=(\"segment\", lambda x: x.mode()[0] if not x.mode().empty else \"Other\")\n",
    ").reset_index()\n",
    "\n",
    "# Ensure output directory exists (redundant but safe)\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the per-video aggregated segments\n",
    "VIDEO_SEGMENTS_PATH = f\"{OUTPUT_DIR}/segments_video.csv\"\n",
    "video_segments.to_csv(VIDEO_SEGMENTS_PATH, index=False)\n",
    "print(f\"[OK] Saved segments_video.csv to {VIDEO_SEGMENTS_PATH}\")\n",
    "print(f\"      Shape: {video_segments.shape}\")\n",
    "print(f\"      Columns: {list(video_segments.columns)}\")\n",
    "print(f\"      Sample rows:\")\n",
    "print(video_segments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52274cce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T13:11:21.823969Z",
     "iopub.status.busy": "2025-08-27T13:11:21.823642Z",
     "iopub.status.idle": "2025-08-27T13:11:21.835449Z",
     "shell.execute_reply": "2025-08-27T13:11:21.834718Z"
    },
    "papermill": {
     "duration": 0.017741,
     "end_time": "2025-08-27T13:11:21.836735",
     "exception": false,
     "start_time": "2025-08-27T13:11:21.818994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model artifacts...\n",
      "[OK] Saved preprocessor to /kaggle/working/artifacts/scaler.pkl\n",
      "[OK] Saved GMM model to /kaggle/working/artifacts/segments_model.pkl\n",
      "[OK] Saved segment mapping to /kaggle/working/artifacts/segment_mapping.pkl\n",
      "[OK] Saved feature specification to /kaggle/working/artifacts/feature_spec.json\n"
     ]
    }
   ],
   "source": [
    "# ===== 9. SAVE MODEL ARTIFACTS =====\n",
    "print(\"[INFO] Saving model artifacts...\")\n",
    "\n",
    "# Ensure artifacts directory exists\n",
    "Path(ARTIFACTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Save the fitted preprocessor (scaler, encoder)\n",
    "PREPROCESSOR_PATH = f\"{ARTIFACTS_DIR}/scaler.pkl\"\n",
    "joblib.dump(preprocessor, PREPROCESSOR_PATH)\n",
    "print(f\"[OK] Saved preprocessor to {PREPROCESSOR_PATH}\")\n",
    "\n",
    "# 2. Save the fitted final GMM model\n",
    "GMM_MODEL_PATH = f\"{ARTIFACTS_DIR}/segments_model.pkl\"\n",
    "joblib.dump(gmm_final, GMM_MODEL_PATH)\n",
    "print(f\"[OK] Saved GMM model to {GMM_MODEL_PATH}\")\n",
    "\n",
    "# 3. Save the segment mapping dictionary\n",
    "SEGMENT_MAPPING_PATH = f\"{ARTIFACTS_DIR}/segment_mapping.pkl\"\n",
    "joblib.dump(segment_mapping, SEGMENT_MAPPING_PATH)\n",
    "print(f\"[OK] Saved segment mapping to {SEGMENT_MAPPING_PATH}\")\n",
    "\n",
    "# 4. Save feature specification for documentation/reproducibility\n",
    "FEATURE_SPEC_PATH = f\"{ARTIFACTS_DIR}/feature_spec.json\"\n",
    "feature_spec = {\n",
    "    \"numeric_features\": numeric_features,\n",
    "    \"categorical_features\": list(categorical_feature_names_out), # Save the actual encoded names\n",
    "    \"final_feature_names\": feature_names, # Full list of features after preprocessing\n",
    "    \"optimal_k\": optimal_k,\n",
    "    \"covariance_type\": \"full\",\n",
    "    \"segment_mapping\": segment_mapping,\n",
    "    \"heuristic_notes\": \"GenZ: High slang, late night, low emoji. Millennial: Daytime, moderate slang/emoji. InterestDriven: High beauty keywords.\"\n",
    "}\n",
    "import json\n",
    "with open(FEATURE_SPEC_PATH, 'w') as f:\n",
    "    json.dump(feature_spec, f, indent=2)\n",
    "print(f\"[OK] Saved feature specification to {FEATURE_SPEC_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88ef904b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T13:11:21.845412Z",
     "iopub.status.busy": "2025-08-27T13:11:21.845073Z",
     "iopub.status.idle": "2025-08-27T13:11:22.106262Z",
     "shell.execute_reply": "2025-08-27T13:11:22.105267Z"
    },
    "papermill": {
     "duration": 0.26713,
     "end_time": "2025-08-27T13:11:22.107620",
     "exception": false,
     "start_time": "2025-08-27T13:11:21.840490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "[INFO] AUDIENCE SEGMENTATION COMPLETE\n",
      "==================================================\n",
      "Input Data: /kaggle/input/data-cleaning/comments_enriched.parquet\n",
      "Final Clusters (k): 4\n",
      "Model Used: Gaussian Mixture Model (GMM)\n",
      "Output Files Generated:\n",
      "  - Per-commenter labels: /kaggle/working/segments_labels.csv\n",
      "  - Per-video aggregation: /kaggle/working/segments_video.csv\n",
      "  - Model artifacts in: /kaggle/working/artifacts\n",
      "------------------------------\n",
      "Segment Distribution:\n",
      "  - Millennial: 1,696,089 (35.90%)\n",
      "  - GenZ: 1,592,642 (33.71%)\n",
      "  - InterestDriven: 1,436,281 (30.40%)\n",
      "==================================================\n",
      "[SUCCESS] You can now proceed to fundamentals.py\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== 10. FINAL REPORTING =====\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"[INFO] AUDIENCE SEGMENTATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input Data: {INPUT_PATH}\")\n",
    "print(f\"Final Clusters (k): {optimal_k}\")\n",
    "print(f\"Model Used: Gaussian Mixture Model (GMM)\")\n",
    "print(f\"Output Files Generated:\")\n",
    "print(f\"  - Per-commenter labels: {SEGMENT_LABELS_PATH}\")\n",
    "print(f\"  - Per-video aggregation: {VIDEO_SEGMENTS_PATH}\")\n",
    "print(f\"  - Model artifacts in: {ARTIFACTS_DIR}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Segment Distribution:\")\n",
    "for segment, count in segment_labels['segment'].value_counts().items():\n",
    "    pct = count / len(segment_labels) * 100\n",
    "    print(f\"  - {segment}: {count:,} ({pct:.2f}%)\")\n",
    "print(\"=\"*50)\n",
    "print(\"[SUCCESS] You can now proceed to fundamentals.py\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8128629,
     "sourceId": 12851871,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 258022563,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 678.190792,
   "end_time": "2025-08-27T13:11:24.538538",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-27T13:00:06.347746",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
